[
  {
    "id": 16,
    "title": "A mathematical framework for transformer circuits",
    "tags": [],
    "primaryTag": "Techniques/General",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "blog": "https://transformer-circuits.pub/2021/framework/index.html"
    }
  },
  {
    "id": 17,
    "title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
    "tags": [],
    "primaryTag": "Techniques/General",
    "date": "2024-01",
    "authors": [
      "Asma Ghandeharioun",
      "Avi Caciularu",
      "Adam Pearce",
      "Lucas Dixon",
      "Mor Geva"
    ],
    "abstract": "Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.",
    "urls": {
      "arxiv 2401": "http://arxiv.org/abs/2401.06102"
    }
  },
  {
    "id": 18,
    "title": "interpreting GPT: the logit lens",
    "tags": [],
    "primaryTag": "Techniques/Embedding Projection",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Lesswrong 2020": "https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"
    }
  },
  {
    "id": 19,
    "title": "Analyzing Transformers in Embedding Space",
    "tags": [],
    "primaryTag": "Techniques/Embedding Projection",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "ACL 2023": "https://aclanthology.org/2023.acl-long.893"
    }
  },
  {
    "id": 20,
    "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens",
    "tags": [],
    "primaryTag": "Techniques/Embedding Projection",
    "date": "2023-03",
    "authors": [
      "Nora Belrose",
      "Zach Furman",
      "Logan Smith",
      "Danny Halawi",
      "Igor Ostrovsky",
      "Lev McKinney",
      "Stella Biderman",
      "Jacob Steinhardt"
    ],
    "abstract": "We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \\emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.\nWe test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at this https URL.",
    "urls": {
      "arxiv 2303": "https://arxiv.org/abs/2303.08112"
    }
  },
  {
    "id": 21,
    "title": "An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l",
    "tags": [],
    "primaryTag": "Techniques/Embedding Projection",
    "date": "2023-10",
    "authors": [
      "Jett Janiak",
      "Can Rager",
      "James Dao",
      "Yeu-Tong Lau"
    ],
    "abstract": "Prior work suggests that language models manage the limited bandwidth of the residual stream through a \"memory management\" mechanism, where certain attention heads and MLP layers clear residual stream directions set by earlier layers. Our study provides concrete evidence for this erasure phenomenon in a 4-layer transformer, identifying heads that consistently remove the output of earlier heads. We further demonstrate that direct logit attribution (DLA), a common technique for interpreting the output of intermediate transformer layers, can show misleading results by not accounting for erasure.",
    "urls": {
      "arxiv 2310": "http://arxiv.org/abs/2310.07325"
    }
  },
  {
    "id": 22,
    "title": "Future Lens: Anticipating Subsequent Tokens from a Single Hidden State",
    "tags": [],
    "primaryTag": "Techniques/Embedding Projection",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "CoNLL 2023": "https://aclanthology.org/2023.conll-1.37/"
    }
  },
  {
    "id": 23,
    "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
    "tags": [],
    "primaryTag": "Techniques/Embedding Projection",
    "date": "2024-03",
    "authors": [
      "Haozhe Chen",
      "Carl Vondrick",
      "Chengzhi Mao"
    ],
    "abstract": "How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.",
    "urls": {
      "arxiv 2403": "https://arxiv.org/abs/2403.10949"
    }
  },
  {
    "id": 24,
    "title": "InversionView: A General-Purpose Method for Reading Information from Neural Activations",
    "tags": [],
    "primaryTag": "Techniques/Embedding Projection",
    "date": "2024",
    "authors": [
      "Xinting Huang",
      "Madhur Panwar",
      "Navin Goyal",
      "Michael Hahn"
    ],
    "abstract": "The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. Computing such subsets is nontrivial as the input space is exponentially large. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present four case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits.",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=P7MW0FahEq"
    }
  },
  {
    "id": 25,
    "title": "Enhancing Neural Network Transparency through Representation Analysis",
    "tags": [],
    "primaryTag": "Techniques/Probing",
    "date": "2023-10",
    "authors": [
      "Andy Zou",
      "Long Phan",
      "Sarah Chen",
      "James Campbell",
      "Phillip Guo",
      "Richard Ren",
      "Alexander Pan",
      "Xuwang Yin",
      "Mantas Mazeika",
      "Ann-Kathrin Dombrowski",
      "Shashwat Goel",
      "Nathaniel Li",
      "Michael J. Byun",
      "Zifan Wang",
      "Alex Mallen",
      "Steven Basart",
      "Sanmi Koyejo",
      "Dawn Song",
      "Matt Fredrikson",
      "J. Zico Kolter",
      "Dan Hendrycks"
    ],
    "abstract": "In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.",
    "urls": {
      "arxiv 2310": "https://arxiv.org/abs/2310.01405",
      "openreview": "https://openreview.net/forum?id=aCgybhcZFi"
    }
  },
  {
    "id": 26,
    "title": "Analyzing And Editing Inner Mechanisms of Backdoored Language Models",
    "tags": [],
    "primaryTag": "Techniques/Causal Intervention",
    "date": "2023-02",
    "authors": [
      "Max Lamparth",
      "Anka Reuel"
    ],
    "abstract": "Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets.\nTrigger warning: Offensive language.",
    "urls": {
      "arxiv 2303": "http://arxiv.org/abs/2302.12461"
    }
  },
  {
    "id": 27,
    "title": "Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations",
    "tags": [],
    "primaryTag": "Techniques/Causal Intervention",
    "date": "2023-03",
    "authors": [
      "Atticus Geiger",
      "Zhengxuan Wu",
      "Christopher Potts",
      "Thomas Icard",
      "Noah D. Goodman"
    ],
    "abstract": "Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS removes previous obstacles to conducting causal abstraction analyses and allows us to find conceptual structure in trained neural nets.",
    "urls": {
      "arxiv 2303": "https://arxiv.org/abs/2303.02536"
    }
  },
  {
    "id": 28,
    "title": "Localizing Model Behavior with Path Patching",
    "tags": [],
    "primaryTag": "Techniques/Causal Intervention",
    "date": "2023-04",
    "authors": [
      "Nicholas Goldowsky-Dill",
      "Chris MacLeod",
      "Lucas Sato",
      "Aryaman Arora"
    ],
    "abstract": "Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.",
    "urls": {
      "arxiv 2304": "https://arxiv.org/abs/2304.05969"
    }
  },
  {
    "id": 29,
    "title": "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca",
    "tags": [],
    "primaryTag": "Techniques/Causal Intervention",
    "date": "2023",
    "authors": [
      "Zhengxuan Wu",
      "Atticus Geiger",
      "Thomas Icard",
      "Christopher Potts",
      "Noah Goodman"
    ],
    "abstract": "",
    "urls": {
      "NIPS 2023": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/f6a8b109d4d4fd64c75e94aaf85d9697-Abstract-Conference.html"
    }
  },
  {
    "id": 30,
    "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods",
    "tags": [],
    "primaryTag": "Techniques/Causal Intervention",
    "date": "2024",
    "authors": [
      "Fred Zhang",
      "Neel Nanda"
    ],
    "abstract": "Mechanistic interpretability seeks to understand the internal mechanisms of\nmachine learning models, where localization—identifying the important model\ncomponents—is a key step. Activation patching, also known as causal tracing or\ninterchange intervention, is a standard technique for this task (Vig et al., 2020), but\nthe literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact\nof methodological details in activation patching, including evaluation metrics and\ncorruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate\ninterpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide\nrecommendations for the best practices of activation patching going forwards.",
    "urls": {
      "ICLR 2024": "https://openreview.net/forum?id=Hf17y6u9BC"
    }
  },
  {
    "id": 31,
    "title": "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching",
    "tags": [],
    "primaryTag": "Techniques/Causal Intervention",
    "date": "2024",
    "authors": [
      "Aleksandar Makelov",
      "Georg Lange",
      "Atticus Geiger",
      "Neel Nanda"
    ],
    "abstract": "Mechanistic interpretability aims to attribute high-level model behaviors to specific, interpretable learned features. It is hypothesized that these features manifest as directions or low-dimensional subspaces within activation space. Accordingly, recent studies have explored the identification and manipulation of such subspaces to reverse-engineer computations, employing methods such as activation patching. In this work, we demonstrate that naïve approaches to subspace interventions can give rise to interpretability illusions.\nSpecifically, even if patching along a subspace has the intended end-to-end causal effect on model behavior, this effect may be achieved by activating \\emph{a dormant parallel pathway} using a component that is \\textit{causally disconnected} from the model output.\nWe demonstrate this in a mathematical example, realize the example empirically in two different settings (the Indirect Object Identification (IOI) task and factual recall), and argue that activating dormant pathways ought to be prevalent in practice.\nIn the context of factual recall, we further show that the illusion is related to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localisation.\nHowever, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability.\nTo contextualize our findings, we also show what a success case looks like in a task (IOI) where prior manual circuit analysis allows an understanding of the location of the ground truth feature. We explore the additional evidence needed to argue that a patched subspace is faithful.",
    "urls": {
      "ICLR 2024": "https://openreview.net/forum?id=Ebt7JgMHv1"
    }
  },
  {
    "id": 32,
    "title": "A Reply to Makelov et al. (2023)'s \"Interpretability Illusion\" Arguments",
    "tags": [],
    "primaryTag": "Techniques/Causal Intervention",
    "date": "2024-01",
    "authors": [
      "Zhengxuan Wu",
      "Atticus Geiger",
      "Jing Huang",
      "Aryaman Arora",
      "Thomas Icard",
      "Christopher Potts",
      "Noah D. Goodman"
    ],
    "abstract": "We respond to the recent paper by Makelov et al. (2023), which reviews subspace interchange intervention methods like distributed alignment search (DAS; Geiger et al. 2023) and claims that these methods potentially cause \"interpretability illusions\". We first review Makelov et al. (2023)'s technical notion of what an \"interpretability illusion\" is, and then we show that even intuitive and desirable explanations can qualify as illusions in this sense. As a result, their method of discovering \"illusions\" can reject explanations they consider \"non-illusory\". We then argue that the illusions Makelov et al. (2023) see in practice are artifacts of their training and evaluation paradigms. We close by emphasizing that, though we disagree with their core characterization, Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the field of interpretability forward.",
    "urls": {
      "arxiv 2401": "https://arxiv.org/abs/2401.12631"
    }
  },
  {
    "id": 33,
    "title": "CausalGym: Benchmarking causal interpretability methods on linguistic tasks",
    "tags": [],
    "primaryTag": "Techniques/Causal Intervention",
    "date": "2024-02",
    "authors": [
      "Aryaman Arora",
      "Dan Jurafsky",
      "Christopher Potts"
    ],
    "abstract": "Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.12560"
    }
  },
  {
    "id": 34,
    "title": "How to use and interpret activation patching",
    "tags": [],
    "primaryTag": "Techniques/Causal Intervention",
    "date": "2024-04",
    "authors": [
      "Stefan Heimersheim",
      "Neel Nanda"
    ],
    "abstract": "Activation patching is a popular mechanistic interpretability technique, but has many subtleties regarding how it is applied and how one may interpret the results. We provide a summary of advice and best practices, based on our experience using this technique in practice. We include an overview of the different ways to apply activation patching and a discussion on how to interpret the results. We focus on what evidence patching experiments provide about circuits, and on the choice of metric and associated pitfalls.",
    "urls": {
      "arxiv 2404": "http://arxiv.org/abs/2404.15255"
    }
  },
  {
    "id": 35,
    "title": "Towards Automated Circuit Discovery for Mechanistic Interpretability",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2023",
    "authors": [
      "Arthur Conmy",
      "Augustine Mavor-Parker",
      "Aengus Lynch",
      "Stefan Heimersheim",
      "Adrià Garriga-Alonso"
    ],
    "abstract": "",
    "urls": {
      "NIPS 2023": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html"
    }
  },
  {
    "id": 36,
    "title": "Neuron to Graph: Interpreting Language Model Neurons at Scale",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2023-05",
    "authors": [
      "Alex Foote",
      "Neel Nanda",
      "Esben Kran",
      "Ioannis Konstas",
      "Shay Cohen",
      "Fazl Barez"
    ],
    "abstract": "Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at this https URL.",
    "urls": {
      "arxiv 2305": "https://arxiv.org/abs/2305.19911",
      "openreview": "https://openreview.net/forum?id=JBLHIR8kBZ"
    }
  },
  {
    "id": 37,
    "title": "Discovering Variable Binding Circuitry with Desiderata",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2023-07",
    "authors": [
      "Xander Davies",
      "Max Nadeau",
      "Nikhil Prakash",
      "Tamar Rott Shaham",
      "David Bau"
    ],
    "abstract": "Recent work has shown that computation in language models may be human-understandable, with successful efforts to localize and intervene on both single-unit features and input-output circuits. Here, we introduce an approach which extends causal mediation experiments to automatically identify model components responsible for performing a specific subtask by solely specifying a set of \\textit{desiderata}, or causal attributes of the model components executing that subtask. As a proof of concept, we apply our method to automatically discover shared \\textit{variable binding circuitry} in LLaMA-13B, which retrieves variable values for multiple arithmetic tasks. Our method successfully localizes variable binding to only 9 attention heads (of the 1.6k) and one MLP in the final token's residual stream.",
    "urls": {
      "arxiv 2307": "http://arxiv.org/abs/2307.03637"
    }
  },
  {
    "id": 38,
    "title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2023",
    "authors": [
      "Deniz Bayazit",
      "Negar Foroutan",
      "Zeming Chen",
      "Gail Weiss",
      "Antoine Bosselut"
    ],
    "abstract": "Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized relational knowledge) but struggles to express the removed knowledge, and suffers performance drops on examples needing this removed knowledge on downstream tasks after finetuning.",
    "urls": {
      "openreview": "https://openreview.net/forum?id=Mkdwvl3Y8L"
    }
  },
  {
    "id": 39,
    "title": "Attribution Patching Outperforms Automated Circuit Discovery",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2023-10",
    "authors": [
      "Aaquib Syed",
      "Can Rager",
      "Arthur Conmy"
    ],
    "abstract": "Automated interpretability research has recently attracted attention as a potential research direction that could scale explanations of neural network behavior to large models. Existing automated circuit discovery work applies activation patching to identify subnetworks responsible for solving specific tasks (circuits). In this work, we show that a simple method based on attribution patching outperforms all existing methods while requiring just two forward passes and a backward pass. We apply a linear approximation to activation patching to estimate the importance of each edge in the computational subgraph. Using this approximation, we prune the least important edges of the network. We survey the performance and limitations of this method, finding that averaged over all tasks our method has greater AUC from circuit recovery than other methods.",
    "urls": {
      "arxiv 2310": "https://arxiv.org/abs/2310.10348"
    }
  },
  {
    "id": 40,
    "title": "AtP*: An efficient and scalable method for localizing LLM behaviour to components",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2024-03",
    "authors": [
      "János Kramár",
      "Tom Lieberum",
      "Rohin Shah",
      "Neel Nanda"
    ],
    "abstract": "Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.",
    "urls": {
      "arxiv 2403": "https://arxiv.org/abs/2403.00745"
    }
  },
  {
    "id": 41,
    "title": "Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2024-03",
    "authors": [
      "Michael Hanna",
      "Sandro Pezzelle",
      "Yonatan Belinkov"
    ],
    "abstract": "Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faithful than those found using EAP-IG, even though both have high node overlap with circuits found previously using causal interventions. We conclude more generally that when using circuits to compare the mechanisms models use to solve tasks, faithfulness, not overlap, is what should be measured.",
    "urls": {
      "arxiv 2403": "http://arxiv.org/abs/2403.17806"
    }
  },
  {
    "id": 42,
    "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2024-03",
    "authors": [
      "Samuel Marks",
      "Can Rager",
      "Eric J. Michaud",
      "Yonatan Belinkov",
      "David Bau",
      "Aaron Mueller"
    ],
    "abstract": "We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.",
    "urls": {
      "arxiv 2403": "https://arxiv.org/abs/2403.19647"
    }
  },
  {
    "id": 43,
    "title": "Automatically Identifying Local and Global Circuits with Linear Computation Graphs",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2024-05",
    "authors": [
      "Xuyang Ge",
      "Fukang Zhu",
      "Wentao Shu",
      "Junxuan Wang",
      "Zhengfu He",
      "Xipeng Qiu"
    ],
    "abstract": "Circuit analysis of any certain model behavior is a central task in mechanistic interpretability. We introduce our circuit discovery pipeline with Sparse Autoencoders (SAEs) and a variant called Transcoders. With these two modules inserted into the model, the model's computation graph with respect to OV and MLP circuits becomes strictly linear. Our methods do not require linear approximation to compute the causal effect of each node. This fine-grained graph identifies both end-to-end and local circuits accounting for either logits or intermediate features. We can scalably apply this pipeline with a technique called Hierarchical Attribution. We analyze three kinds of circuits in GPT-2 Small: bracket, induction, and Indirect Object Identification circuits. Our results reveal new findings underlying existing discoveries.",
    "urls": {
      "arxiv 2405": "https://arxiv.org/abs/2405.13868"
    }
  },
  {
    "id": 44,
    "title": "Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2024-05",
    "authors": [
      "Charles O'Neill",
      "Thang Bui"
    ],
    "abstract": "This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity and sensitivity to hyperparameters. We propose training sparse autoencoders on carefully designed positive and negative examples, where the model can only correctly predict the next token for the positive examples. We hypothesise that learned representations of attention head outputs will signal when a head is engaged in specific computations. By discretising the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each head, we enable direct identification of attention heads involved in circuits without the need for expensive ablations or architectural modifications. On three well-studied tasks - indirect object identification, greater-than comparisons, and docstring completion - the proposed method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds. Notably, we require only 5-10 text examples for each task to learn robust representations. Our findings highlight the promise of discrete sparse autoencoders for scalable and efficient mechanistic interpretability, offering a new direction for analysing the inner workings of large language models.",
    "urls": {
      "arxiv 2405": "http://arxiv.org/abs/2405.12522"
    }
  },
  {
    "id": 45,
    "title": "Hypothesis Testing the Circuit Hypothesis in LLMs",
    "tags": [],
    "primaryTag": "Techniques/Automation",
    "date": "2024",
    "authors": [
      "Claudia Shi",
      "Nicolas Beltran-Velez",
      "Achille Nazaret",
      "Carolina Zheng",
      "Adrià Garriga-Alonso",
      "Andrew Jesson",
      "Maggie Makar",
      "David Blei"
    ],
    "abstract": "Large language models (LLMs) demonstrate surprising capabilities, but we do not understand how they are implemented. One hypothesis suggests that these capabilities are primarily executed by small subnetworks within the LLM, known as circuits. But how can we evaluate this hypothesis? In this paper, we formalize a set of criteria that a circuit is hypothesized to meet and develop a suite of hypothesis tests to evaluate how well circuits satisfy them. The criteria focus on the extent to which the LLM's behavior is preserved, the degree of localization of this behavior, and whether the circuit is minimal. We apply these tests to six circuits described in the research literature. We find that synthetic circuits -- circuits that are hard-coded in the model -- align with the idealized properties. Circuits discovered in Transformer models satisfy the criteria to varying degrees.",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=ibSNv9cldu"
    }
  },
  {
    "id": 46,
    "title": "Towards monosemanticity: Decomposing language models with dictionary learning",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Transformer Circuits Thread": "https://transformer-circuits.pub/2023/monosemantic-features"
    }
  },
  {
    "id": 47,
    "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024",
    "authors": [
      "Robert Huben",
      "Hoagy Cunningham",
      "Logan Riggs Smith",
      "Aidan Ewart",
      "Lee Sharkey"
    ],
    "abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",
    "urls": {
      "ICLR 2024": "https://openreview.net/forum?id=F76bwRSLeK"
    }
  },
  {
    "id": 48,
    "title": "Open Source Sparse Autoencoders for all Residual Stream Layers of GPT2-Small",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Alignment Forum": "https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream"
    }
  },
  {
    "id": 49,
    "title": "Attention SAEs Scale to GPT-2 Small",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Alignment Forum": "https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr/attention-saes-scale-to-gpt-2-small"
    }
  },
  {
    "id": 50,
    "title": "We Inspected Every Head In GPT-2 Small using SAEs So You Don't Have To",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Alignment Forum": "https://www.alignmentforum.org/posts/xmegeW5mqiBsvoaim/we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don"
    }
  },
  {
    "id": 51,
    "title": "Understanding SAE Features with the Logit Lens",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Alignment Forum": "https://www.alignmentforum.org/posts/qykrYY6rXXM7EEs8Q/understanding-sae-features-with-the-logit-lens"
    }
  },
  {
    "id": 52,
    "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Transformer Circuits Thread": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    }
  },
  {
    "id": 53,
    "title": "Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024-05",
    "authors": [
      "Charles O'Neill",
      "Thang Bui"
    ],
    "abstract": "This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity and sensitivity to hyperparameters. We propose training sparse autoencoders on carefully designed positive and negative examples, where the model can only correctly predict the next token for the positive examples. We hypothesise that learned representations of attention head outputs will signal when a head is engaged in specific computations. By discretising the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each head, we enable direct identification of attention heads involved in circuits without the need for expensive ablations or architectural modifications. On three well-studied tasks - indirect object identification, greater-than comparisons, and docstring completion - the proposed method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds. Notably, we require only 5-10 text examples for each task to learn robust representations. Our findings highlight the promise of discrete sparse autoencoders for scalable and efficient mechanistic interpretability, offering a new direction for analysing the inner workings of large language models.",
    "urls": {
      "arxiv 2405": "http://arxiv.org/abs/2405.12522"
    }
  },
  {
    "id": 54,
    "title": "Scaling and evaluating sparse autoencoders",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024-06",
    "authors": [
      "Leo Gao",
      "Tom Dupré la Tour",
      "Henk Tillman",
      "Gabriel Goh",
      "Rajan Troll",
      "Alec Radford",
      "Ilya Sutskever",
      "Jan Leike",
      "Jeffrey Wu"
    ],
    "abstract": "Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.",
    "urls": {
      "arxiv 2406": "https://arxiv.org/abs/2406.04093",
      "code": "https://github.com/openai/sparse_autoencoder/"
    }
  },
  {
    "id": 55,
    "title": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024",
    "authors": [
      "Adam Karvonen",
      "Benjamin Wright",
      "Can Rager",
      "Rico Angell",
      "Jannik Brinkmann",
      "Logan Riggs Smith",
      "Claudio Mayrink Verdun",
      "David Bau",
      "Samuel Marks"
    ],
    "abstract": "What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. However, evaluating the quality of these SAEs is difficult because we lack a ground-truth collection of interpretable features that we expect good SAEs to recover. We thus propose to measure progress in interpretable dictionary learning by working in the setting of LMs trained on chess and Othello transcripts. These settings carry natural collections of interpretable features---for example, ``there is a knight on F3''---which we leverage into \\textit{supervised} metrics for SAE quality. To guide progress in interpretable dictionary learning, we introduce a new SAE training technique, \\textit{$p$-annealing}, which improves performance on prior unsupervised metrics as well as our new metrics.",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=qzsDKwGJyB"
    }
  },
  {
    "id": 56,
    "title": "Sparse Autoencoders Match Supervised Features for Model Steering on the IOI Task",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024",
    "authors": [
      "Aleksandar Makelov"
    ],
    "abstract": "Sparse autoencoders (SAEs) have attracted atten-\ntion as a way towards unsupervised disentangling\nof hidden LLM activations into meaningful fea-\ntures. However, evaluations of SAE architectures\nand training algorithms have so far been indi-\nrect due to the difficulty – both conceptual and\ntechnical – of obtaining ‘ground truth’ features\nto compare against. To overcome this, recent\nwork (Makelov et al., 2024) has proposed a suite\nof SAE evaluations that compare SAE features\nagainst feature dictionaries learned with super-\nvision for a specific model capability. However,\nthe evaluations were implemented in a mostly ex-\nploratory way, and did not optimize for eliciting\nbest SAE performance across different SAE vari-\nants.\nWhile initial results are promising, they rely on qualitative\nand/or indirect evaluation of the learned features such as\nproxies for the ‘true’ features, non-trivial assumptions about\nSAE learning or success in toy models (Elhage et al., 2022;\nBricken et al., 2023; Sharkey et al., 2023). As a step to-\nwards more objective SAE evaluations, recently Makelov\net al. (2024) proposed to use sparse feature dictionaries\nlearned with supervision in the context of a given model\ncapability (specifically, the IOI task (Wang et al., 2023)) as\na ‘skyline’ for achievable SAE performance w.r.t. this capa-\nbility. They developed several evaluations that (1) confirm\nthe supervised features provide a high-quality decomposi-\ntion of model computations w.r.t the capability and (2) use\nthese supervised features to contextualize SAE results, for\nSAEs trained on distributions of either capability-specific\nor internet text.\nIn this work, we improve upon this by running\na systematic and thorough study of using SAEs\nfor steering on the IOI task, comparing several\nrecently proposed SAE variants: ‘vanilla’ SAEs\n(Bricken et al., 2023), gated SAEs (Rajamanoha-\nran et al., 2024) and topK SAEs (Gao et al., 2024).\nWe find that, even by employing a simple and\ncheap heuristic for choosing good SAEs for edit-\ning, we are able to greatly improve upon the re-\nsults of prior work, and demonstrate that SAE\nfeatures are able to perform on par with super-\nvised feature dictionaries. Further, we find that\ntopK SAEs and gated SAEs generally outperform\nother variants on this test, and topK SAEs can\nalmost match supervised features in terms of edit\nquality.",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=JdrVuEQih5"
    }
  },
  {
    "id": 57,
    "title": "Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024",
    "authors": [
      "Dan Braun",
      "Jordan Taylor",
      "Nicholas Goldowsky-Dill",
      "Lee Sharkey"
    ],
    "abstract": "Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network's internal activations, have been used to identify these features. However, SAEs may learn more about the structure of the datatset than the computational structure of the network. There is therefore only indirect reason to believe that the directions found in these dictionaries are functionally important to the network. We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a Pareto improvement: They explain more network performance, require fewer total features, and require fewer simultaneously active features per datapoint, all with no cost to interpretability. We explore geometric and qualitative differences between e2e SAE features and standard SAE features. E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately. We release our library for training e2e SAEs and reproducing our analysis at https://github.com/ApolloResearch/e2e_sae.",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=bcV7rhBEcM"
    }
  },
  {
    "id": 58,
    "title": "Transcoders find interpretable LLM feature circuits",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024",
    "authors": [
      "Jacob Dunefsky",
      "Philippe Chlenski",
      "Neel Nanda"
    ],
    "abstract": "A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features---such as those found by sparse autoencoders (SAEs)---are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for. Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore transcoders, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We successfully train transcoders on language models with 120M, 410M, and 1.4B parameters, and find them to perform at least on par with SAEs in terms of sparsity, faithfulness, and human-interpretability. We then introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers. The resulting circuits neatly factorize into input-dependent and input-invariant terms. Finally, we apply transcoders to reverse-engineer unknown circuits in the model, and we obtain novel insights regarding the \"greater-than circuit\" in GPT2-small. Our results suggest that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits. Code is available at https://github.com/jacobdunefsky/transcoder_circuits.",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=GWqzUR2dOX"
    }
  },
  {
    "id": 59,
    "title": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024-07",
    "authors": [
      "Senthooran Rajamanoharan",
      "Tom Lieberum",
      "Nicolas Sonnerat",
      "Arthur Conmy",
      "Vikrant Varma",
      "János Kramár",
      "Neel Nanda"
    ],
    "abstract": "Sparse autoencoders (SAEs) are a promising unsupervised approach for identifying causally relevant and interpretable linear features in a language model's (LM) activations. To be useful for downstream tasks, SAEs need to decompose LM activations faithfully; yet to be interpretable the decomposition must be sparse -- two objectives that are in tension. In this paper, we introduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity at a given sparsity level on Gemma 2 9B activations, compared to other recent advances such as Gated and TopK SAEs. We also show that this improvement does not come at the cost of interpretability through manual and automated interpretability studies. JumpReLU SAEs are a simple modification of vanilla (ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU activation function -- and are similarly efficient to train and run. By utilising straight-through-estimators (STEs) in a principled manner, we show how it is possible to train JumpReLU SAEs effectively despite the discontinuous JumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs to directly train L0 to be sparse, instead of training on proxies such as L1, avoiding problems like shrinkage.",
    "urls": {
      "arxiv 2407": "http://arxiv.org/abs/2407.14435"
    }
  },
  {
    "id": 60,
    "title": "Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024-10",
    "authors": [
      "Can Demircan",
      "Tankred Saanum",
      "Akshay K. Jagadish",
      "Marcel Binz",
      "Eric Schulz"
    ],
    "abstract": "In-context learning, the ability to adapt based on a few examples in the input prompt, is a ubiquitous feature of large language models (LLMs). However, as LLMs' in-context learning abilities continue to improve, understanding this phenomenon mechanistically becomes increasingly important. In particular, it is not well-understood how LLMs learn to solve specific classes of problems, such as reinforcement learning (RL) problems, in-context. Through three different tasks, we first show that Llama 33 7070B can solve simple RL problems in-context. We then analyze the residual stream of Llama using Sparse Autoencoders (SAEs) and find representations that closely match temporal difference (TD) errors. Notably, these representations emerge despite the model only being trained to predict the next token. We verify that these representations are indeed causally involved in the computation of TD errors and QQ-values by performing carefully designed interventions on them. Taken together, our work establishes a methodology for studying and manipulating in-context learning with SAEs, paving the way for a more mechanistic understanding.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.01280"
    }
  },
  {
    "id": 61,
    "title": "Mechanistic Permutability: Match Features Across Layers",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024-10",
    "authors": [
      "Nikita Balagansky",
      "Ian Maksimov",
      "Daniil Gavrilov"
    ],
    "abstract": "Understanding how features evolve across layers in deep neural networks is a fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual layers, aligning these features across layers has remained an open problem. In this paper, we introduce SAE Match, a novel, data-free method for aligning SAE features across different layers of a neural network. Our approach involves matching features by minimizing the mean squared error between the folded parameters of SAEs, a technique that incorporates activation thresholds into the encoder and decoder weights to account for differences in feature scales. Through extensive experiments on the Gemma 2 language model, we demonstrate that our method effectively captures feature evolution across layers, improving feature matching quality. We also show that features persist over several layers and that our approach can approximate hidden states across layers. Our work advances the understanding of feature dynamics in neural networks and provides a new tool for mechanistic interpretability studies.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.07656"
    }
  },
  {
    "id": 62,
    "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024-10",
    "authors": [
      "Michael Lan",
      "Philip Torr",
      "Austin Meek",
      "Ashkan Khakzar",
      "David Krueger",
      "Fazl Barez"
    ],
    "abstract": "We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones, making it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics on SAE feature spaces across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.06981"
    }
  },
  {
    "id": 63,
    "title": "Investigating Sensitive Directions in GPT-2: An Improved Baseline and Comparative Analysis of SAEs",
    "tags": [],
    "primaryTag": "Techniques/Sparse Coding",
    "date": "2024-10",
    "authors": [
      "Daniel J. Lee",
      "Stefan Heimersheim"
    ],
    "abstract": "Sensitive directions experiments attempt to understand the computational features of Language Models (LMs) by measuring how much the next token prediction probabilities change by perturbing activations along specific directions. We extend the sensitive directions work by introducing an improved baseline for perturbation directions. We demonstrate that KL divergence for Sparse Autoencoder (SAE) reconstruction errors are no longer pathologically high compared to the improved baseline. We also show that feature directions uncovered by SAEs have varying impacts on model outputs depending on the SAE's sparsity, with lower L0 SAE feature directions exerting a greater influence. Additionally, we find that end-to-end SAE features do not exhibit stronger effects on model outputs compared to traditional SAEs.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.12555"
    }
  },
  {
    "id": 64,
    "title": "Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT",
    "tags": [],
    "primaryTag": "Techniques/Visualization",
    "date": "2023-05",
    "authors": [
      "Shahar Katz",
      "Yonatan Belinkov"
    ],
    "abstract": "Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them more human interpretable. In this paper, we investigate LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on our discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that can reflect the models' internal processing, uncovering the contribution of each component to the models' final prediction. Our visualization also unveils new insights about the role of layer norms as semantic filters that influence the models' output, and about neurons that are always activated during forward passes and act as regularization vectors.",
    "urls": {
      "arxiv 2305": "[10.48550/arXiv.2305.13417](http://arxiv.org/abs/2305.13417)",
      "github": "https://github.com/shacharKZ/Visualizing-the-Information-Flow-of-GPT"
    }
  },
  {
    "id": 65,
    "title": "Sparse AutoEncoder Visulization",
    "tags": [],
    "primaryTag": "Techniques/Visualization",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "github": "https://github.com/callummcdougall/sae_vis"
    }
  },
  {
    "id": 66,
    "title": "SAE-VIS: Announcement Post",
    "tags": [],
    "primaryTag": "Techniques/Visualization",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "lesswrong": "https://www.lesswrong.com/posts/nAhy6ZquNY7AD3RkD/sae-vis-announcement-post-1"
    }
  },
  {
    "id": 67,
    "title": "LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models",
    "tags": [],
    "primaryTag": "Techniques/Visualization",
    "date": "2024-04",
    "authors": [
      "Igor Tufanov",
      "Karen Hambardzumyan",
      "Javier Ferrando",
      "Elena Voita"
    ],
    "abstract": "We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (1) shows the important part of the whole input-to-output information flow, (2) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (3) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications.",
    "urls": {
      "arxiv 2404": "http://arxiv.org/abs/2404.07004",
      "github": "https://github.com/facebookresearch/ llm-transparency-tool"
    }
  },
  {
    "id": 68,
    "title": "Tracr: Compiled Transformers as a Laboratory for Interpretability",
    "tags": [],
    "primaryTag": "Techniques/Translation",
    "date": "2023-01",
    "authors": [
      "David Lindner",
      "János Kramár",
      "Sebastian Farquhar",
      "Matthew Rahtz",
      "Thomas McGrath",
      "Vladimir Mikulik"
    ],
    "abstract": "We show how to \"compile\" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study \"superposition\" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the \"programs\" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at this https URL.",
    "urls": {
      "arxiv 2301": "http://arxiv.org/abs/2301.05062"
    }
  },
  {
    "id": 69,
    "title": "Opening the AI black box: program synthesis via mechanistic interpretability",
    "tags": [],
    "primaryTag": "Techniques/Translation",
    "date": "2024-02",
    "authors": [
      "Eric J. Michaud",
      "Isaac Liao",
      "Vedang Lad",
      "Ziming Liu",
      "Anish Mudide",
      "Chloe Loughridge",
      "Zifan Carl Guo",
      "Tara Rezaei Kheirkhah",
      "Mateja Vukelić",
      "Max Tegmark"
    ],
    "abstract": "We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.05110"
    }
  },
  {
    "id": 70,
    "title": "An introduction to graphical tensor notation for mechanistic interpretability",
    "tags": [],
    "primaryTag": "Techniques/Translation",
    "date": "2024-02",
    "authors": [
      "Jordan K. Taylor"
    ],
    "abstract": "Graphical tensor notation is a simple way of denoting linear operations on tensors, originating from physics. Modern deep learning consists almost entirely of operations on or between tensors, so easily understanding tensor operations is quite important for understanding these systems. This is especially true when attempting to reverse-engineer the algorithms learned by a neural network in order to understand its behavior: a field known as mechanistic interpretability. It's often easy to get confused about which operations are happening between tensors and lose sight of the overall structure, but graphical tensor notation makes it easier to parse things at a glance and see interesting equivalences. The first half of this document introduces the notation and applies it to some decompositions (SVD, CP, Tucker, and tensor network decompositions), while the second half applies it to some existing some foundational approaches for mechanistically understanding language models, loosely following ``A Mathematical Framework for Transformer Circuits'', then constructing an example ``induction head'' circuit in graphical tensor notation.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.01790"
    }
  },
  {
    "id": 71,
    "title": "Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks in Language Models",
    "tags": [],
    "primaryTag": "Techniques/Benchmark",
    "date": "2023-12",
    "authors": [
      "Alexandre Variengien",
      "Eric Winsor"
    ],
    "abstract": "When solving challenging problems, language models (LMs) are able to identify relevant information from long and complicated contexts. To study how LMs solve retrieval tasks in diverse situations, we introduce ORION, a collection of structured retrieval tasks spanning six domains, from text understanding to coding. Each task in ORION can be represented abstractly by a request (e.g. a question) that retrieves an attribute (e.g. the character name) from a context (e.g. a story). We apply causal analysis on 18 open-source language models with sizes ranging from 125 million to 70 billion parameters. We find that LMs internally decompose retrieval tasks in a modular way: middle layers at the last token position process the request, while late layers retrieve the correct entity from the context. After causally enforcing this decomposition, models are still able to solve the original task, preserving 70% of the original correct token probability in 98 of the 106 studied model-task pairs. We connect our macroscopic decomposition with a microscopic description by performing a fine-grained case study of a question-answering task on Pythia-2.8b. Building on our high-level understanding, we demonstrate a proof of concept application for scalable internal oversight of LMs to mitigate prompt-injection while requiring human supervision on only a single input. Our solution improves accuracy drastically (from 15.5% to 97.5% on Pythia-12b). This work presents evidence of a universal emergent modular processing of tasks across varied domains and models and is a pioneering effort in applying interpretability for scalable internal oversight of LMs.",
    "urls": {
      "arxiv 2312": "http://arxiv.org/abs/2312.10091"
    }
  },
  {
    "id": 72,
    "title": "RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations",
    "tags": [],
    "primaryTag": "Techniques/Benchmark",
    "date": "2024-02",
    "authors": [
      "Jing Huang",
      "Zhengxuan Wu",
      "Christopher Potts",
      "Mor Geva",
      "Atticus Geiger"
    ],
    "abstract": "Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at this https URL.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.17700"
    }
  },
  {
    "id": 73,
    "title": "Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control",
    "tags": [],
    "primaryTag": "Techniques/Benchmark",
    "date": "2024-05",
    "authors": [
      "Aleksandar Makelov",
      "George Lange",
      "Neel Nanda"
    ],
    "abstract": "Disentangling model activations into meaningful features is a central problem in interpretability. However, the absence of ground-truth for these features in realistic scenarios makes validating recent approaches, such as sparse dictionary learning, elusive. To address this challenge, we propose a framework for evaluating feature dictionaries in the context of specific tasks, by comparing them against \\emph{supervised} feature dictionaries. First, we demonstrate that supervised dictionaries achieve excellent approximation, control, and interpretability of model computations on the task. Second, we use the supervised dictionaries to develop and contextualize evaluations of unsupervised dictionaries along the same three axes.\nWe apply this framework to the indirect object identification (IOI) task using GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI or OpenWebText datasets. We find that these SAEs capture interpretable features for the IOI task, but they are less successful than supervised features in controlling the model. Finally, we observe two qualitative phenomena in SAE training: feature occlusion (where a causally relevant concept is robustly overshadowed by even slightly higher-magnitude ones in the learned features), and feature over-splitting (where binary features split into many smaller, less interpretable features). We hope that our framework will provide a useful step towards more objective and grounded evaluations of sparse dictionary learning methods.",
    "urls": {
      "arxiv 2405": "http://arxiv.org/abs/2405.08366"
    }
  },
  {
    "id": 74,
    "title": "InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques",
    "tags": [],
    "primaryTag": "Techniques/Benchmark",
    "date": "2024-07",
    "authors": [
      "Rohan Gupta",
      "Iván Arcuschin",
      "Thomas Kwa",
      "Adrià Garriga-Alonso"
    ],
    "abstract": "Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents InterpBench, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr's original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.",
    "urls": {
      "arxiv 2407": "http://arxiv.org/abs/2407.14494"
    }
  },
  {
    "id": 75,
    "title": "Circuit Component Reuse Across Tasks in Transformer Language Models",
    "tags": [],
    "primaryTag": "Ability/General",
    "date": "2024",
    "authors": [
      "Jack Merullo",
      "Carsten Eickhoff",
      "Ellie Pavlick"
    ],
    "abstract": "Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in (Wang, 2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to ‘repair’ the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.",
    "urls": {
      "ICLR 2024 spotlight": "https://openreview.net/forum?id=fpoAYV6Wsk"
    }
  },
  {
    "id": 76,
    "title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures",
    "tags": [],
    "primaryTag": "Ability/General",
    "date": "2024-10",
    "authors": [
      "Junxuan Wang",
      "Xuyang Ge",
      "Wentao Shu",
      "Qiong Tang",
      "Yunhua Zhou",
      "Zhengfu He",
      "Xipeng Qiu"
    ],
    "abstract": "The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely Transformers and Mambas, to explore the extent of their mechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate interpretable features from these models and show that most features are similar in these two models. We also validate the correlation between feature similarity and Universality. We then delve into the circuit-level analysis of Mamba models and find that the induction circuits in Mamba are structurally analogous to those in Transformers. We also identify a nuanced difference we call \\emph{Off-by-One motif}: The information of one token is written into the SSM state in its next position. Whilst interaction between tokens in Transformers does not exhibit such trend.",
    "urls": {
      "arxvi 2410": "https://arxiv.org/abs/2410.06672"
    }
  },
  {
    "id": 77,
    "title": "From Tokens to Words: On the Inner Lexicon of LLMs",
    "tags": [],
    "primaryTag": "Ability/General",
    "date": "2024-10",
    "authors": [
      "Guy Kaplan",
      "Matanel Oren",
      "Yuval Reif",
      "Roy Schwartz"
    ],
    "abstract": "Natural language is composed of words, but modern large language models (LLMs) process sub-words as input. A natural question raised by this discrepancy is whether LLMs encode words internally, and if so how. We present evidence that LLMs engage in an intrinsic detokenization process, where sub-word sequences are combined into coherent whole-word representations at their last token. Our experiments show that this process primarily takes place within the early and middle layers of the model. We further demonstrate its robustness to arbitrary splits (e.g., \"cats\" to \"ca\" and \"ts\"), typos, and importantly-to out-of-vocabulary words: when feeding the last token internal representations of such words to the model as input, it can \"understand\" them as the complete word despite never seeing such representations as input during training. Our findings suggest that LLMs maintain a latent vocabulary beyond the tokenizer's scope. These insights provide a practical, finetuning-free application for expanding the vocabulary of pre-trained models. By enabling the addition of new vocabulary words, we reduce input length and inference iterations, which reduces both space and model latency, with little to no loss in model accuracy.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.05864"
    }
  },
  {
    "id": 78,
    "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
    "tags": [],
    "primaryTag": "Ability/Reasoning",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "EMNLP 2023": "https://aclanthology.org/2023.emnlp-main.299"
    }
  },
  {
    "id": 79,
    "title": "How Large Language Models Implement Chain-of-Thought?",
    "tags": [],
    "primaryTag": "Ability/Reasoning",
    "date": "2023",
    "authors": [
      "Yiqun Wang",
      "Sile Hu",
      "Yonggang Zhang",
      "Xiang Tian",
      "Xuesong Liu",
      "Yaowu Chen",
      "Xu Shen",
      "Jieping Ye"
    ],
    "abstract": "Chain-of-thought (CoT) prompting has showcased the significant enhancement in the reasoning capabilities of large language models (LLMs). Unfortunately, the underlying mechanism behind how CoT prompting works remains elusive. Advanced works show the possibility of revealing the reasoning mechanism of LLMs by leveraging counterfactual examples (CEs) to do a causal intervention. Specifically, analyzing the difference between effects caused by original examples (OEs) and CEs can identify the key attention heads related to the ongoing task, e.g., a reasoning task. However, the completion of reasoning tasks involves diverse abilities of language models such as numerical computation, knowledge retrieval, and logical reasoning, posing challenges to constructing proper CEs.\nIn this work, we propose an in-context learning approach to construct the pair of OEs and CEs, where OEs can activate the reasoning behavior and CEs are similar to OEs but without activating the reasoning behavior. To accurately locate the key heads, we further propose a word of interest (WOI) normalization approach to focus on specific words related to the ground-truth answer. Our empirical observations show that only a small fraction of attention heads contribute to the reasoning task, primarily located in the middle and upper layers of LLMs. Intervention with these identified heads can significantly hamper the model's performance on reasoning tasks. Among these heads, we found that some play a key role in judging for final answer, some play a key role in synthesizing the step-by-step thoughts to get answers, which corresponds to the two stages of the chain-of-thought (CoT) process: firstly think step-by-step to get intermediate thoughts, then answer the question based on these thoughts.",
    "urls": {
      "openreview": "https://openreview.net/forum?id=b2XfOm3RJa"
    }
  },
  {
    "id": 80,
    "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?",
    "tags": [],
    "primaryTag": "Ability/Reasoning",
    "date": "2024-02",
    "authors": [
      "Sohee Yang",
      "Elena Gribovskaya",
      "Nora Kassner",
      "Mor Geva",
      "Sebastian Riedel"
    ],
    "abstract": "We study whether Large Language Models (LLMs) latently perform multi-hop reasoning with complex prompts such as \"The mother of the singer of 'Superstition' is\". We look for evidence of a latent reasoning pathway where an LLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to complete the prompt. We analyze these two hops individually and consider their co-occurrence as indicative of latent multi-hop reasoning. For the first hop, we test if changing the prompt to indirectly mention the bridge entity instead of any other entity increases the LLM's internal recall of the bridge entity. For the second hop, we test if increasing this recall causes the LLM to better utilize what it knows about the bridge entity. We find strong evidence of latent multi-hop reasoning for the prompts of certain relation types, with the reasoning pathway used in more than 80% of the prompts. However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop. Our experimental findings suggest potential challenges and opportunities for future development and applications of LLMs.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.16837"
    }
  },
  {
    "id": 81,
    "title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
    "tags": [],
    "primaryTag": "Ability/Reasoning",
    "date": "2024-02",
    "authors": [
      "Subhabrata Dutta",
      "Joykirat Singh",
      "Soumen Chakrabarti",
      "Tanmoy Chakraborty"
    ],
    "abstract": "Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of Llama-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context prior taking over in the later half. This internal phase shift manifests in different functional components: attention heads that write the answer token appear in the later half, attention heads that move information along ontological relationships appear in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.18312"
    }
  },
  {
    "id": 82,
    "title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning",
    "tags": [],
    "primaryTag": "Ability/Reasoning",
    "date": "2024-02",
    "authors": [
      "Jiachun Li",
      "Pengfei Cao",
      "Chenhao Wang",
      "Zhuoran Jin",
      "Yubo Chen",
      "Daojian Zeng",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.18344"
    }
  },
  {
    "id": 83,
    "title": "Iteration Head: A Mechanistic Study of Chain-of-Thought",
    "tags": [],
    "primaryTag": "Ability/Reasoning",
    "date": "2024-06",
    "authors": [
      "Vivien Cabannes",
      "Charles Arnal",
      "Wassim Bouaziz",
      "Alice Yang",
      "Francois Charton",
      "Julia Kempe"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning is known to improve Large Language Models both empirically and in terms of theoretical approximation power. However, our understanding of the inner workings and conditions of apparition of CoT capabilities remains limited. This paper helps fill this gap by demonstrating how CoT reasoning emerges in transformers in a controlled and interpretable setting. In particular, we observe the appearance of a specialized attention mechanism dedicated to iterative reasoning, which we coined \"iteration heads\". We track both the emergence and the precise working of these iteration heads down to the attention level, and measure the transferability of the CoT skills to which they give rise between tasks.",
    "urls": {
      "arxiv 2406": "https://arxiv.org/abs/2406.02128"
    }
  },
  {
    "id": 84,
    "title": "From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency",
    "tags": [],
    "primaryTag": "Ability/Reasoning",
    "date": "2024-10",
    "authors": [
      "Kaiyue Wen",
      "Huaqing Zhang",
      "Hongzhou Lin",
      "Jingzhao Zhang"
    ],
    "abstract": "Chain-of-thought (CoT) significantly enhances the reasoning performance of large language models (LLM). While current theoretical studies often attribute this improvement to increased expressiveness and computational capacity, we argue that expressiveness is not the primary limitation in the LLM regime, as current large models will fail on simple tasks. Using a parity-learning setup, we demonstrate that CoT can substantially improve sample efficiency even when the representation power is sufficient. Specifically, with CoT, a transformer can learn the function within polynomial samples, whereas without CoT, the required sample size is exponential. Additionally, we show that CoT simplifies the learning process by introducing sparse sequential dependencies among input tokens, and leads to a sparse and interpretable attention. We validate our theoretical analysis with both synthetic and real-world experiments, confirming that sparsity in attention layers is a key factor of the improvement induced by CoT.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.05459"
    }
  },
  {
    "id": 85,
    "title": "Interpretability in the wild: a circuit for indirect object identification in GPT-2 small",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "",
    "authors": [],
    "abstract": "Anonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
    "urls": {
      "ICLR 2023": "https://openreview.net/forum?id=NpsVSN6o4ul"
    }
  },
  {
    "id": 86,
    "title": "Entity Tracking in Language Models",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "ACL 2023": "https://aclanthology.org/2023.acl-long.213"
    }
  },
  {
    "id": 87,
    "title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2023",
    "authors": [
      "Michael Hanna",
      "Ollie Liu",
      "Alexandre Variengien"
    ],
    "abstract": "",
    "urls": {
      "NIPS 2023": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/efbba7719cc5172d175240f24be11280-Abstract-Conference.html"
    }
  },
  {
    "id": 88,
    "title": "Can Transformers Learn to Solve Problems Recursively?",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2023-05",
    "authors": [
      "Shizhuo Dylan Zhang",
      "Curt Tigges",
      "Stella Biderman",
      "Maxim Raginsky",
      "Talia Ringer"
    ],
    "abstract": "Neural networks have in recent years shown promise for helping software engineers write programs and even formally verify them. While semantic information plays a crucial part in these processes, it remains unclear to what degree popular neural architectures like transformers are capable of modeling that information. This paper examines the behavior of neural networks learning algorithms relevant to programs and formal verification proofs through the lens of mechanistic interpretability, focusing in particular on structural recursion. Structural recursion is at the heart of tasks on which symbolic tools currently outperform neural models, like inferring semantic relations between datatypes and emulating program behavior. We evaluate the ability of transformer models to learn to emulate the behavior of structurally recursive functions from input-output examples. Our evaluation includes empirical and conceptual analyses of the limitations and capabilities of transformer models in approximating these functions, as well as reconstructions of the ``shortcut\" algorithms the model learns. By reconstructing these algorithms, we are able to correctly predict 91 percent of failure cases for one of the approximated functions. Our work provides a new foundation for understanding the behavior of neural networks that fail to solve the very tasks they are trained for.",
    "urls": {
      "arxiv 2305": "http://arxiv.org/abs/2305.14699"
    }
  },
  {
    "id": 89,
    "title": "Analyzing And Editing Inner Mechanisms of Backdoored Language Models",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2023",
    "authors": [
      "Max Lamparth",
      "Ann-Katrin Reuel"
    ],
    "abstract": "Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets.\nTrigger warning: Offensive language.",
    "urls": {
      "NeurIPS 2023 Workshop": "https://openreview.net/forum?id=e9F4fB23o0"
    }
  },
  {
    "id": 90,
    "title": "Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2023-07",
    "authors": [
      "Tom Lieberum",
      "Matthew Rahtz",
      "János Kramár",
      "Neel Nanda",
      "Geoffrey Irving",
      "Rohin Shah",
      "Vladimir Mikulik"
    ],
    "abstract": "\\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \\emph{label} given knowledge of the correct answer \\emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).\nWe further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, key and value subspaces of the head without loss of performance when operating on the answer labels for multiple-choice questions, and we show that the query and key subspaces represent an `Nth item in an enumeration' feature to at least some extent. However, when we attempt to use this explanation to understand the heads' behaviour on a more general distribution including randomized answer labels, we find that it is only a partial explanation, suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering.",
    "urls": {
      "arxiv 2307": "http://arxiv.org/abs/2307.09458"
    }
  },
  {
    "id": 91,
    "title": "Refusal mechanisms: initial experiments with Llama-2-7b-chat",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "AlignmentForum 2312": "https://www.alignmentforum.org/posts/pYcEhoAoPfHhgJ8YC/refusal-mechanisms-initial-experiments-with-llama-2-7b-chat"
    }
  },
  {
    "id": 92,
    "title": "Forbidden Facts: An Investigation of Competing Objectives in Llama-2",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2023-12",
    "authors": [
      "Tony T. Wang",
      "Miles Wang",
      "Kaivalya Hariharan",
      "Nir Shavit"
    ],
    "abstract": "LLMs often face competing pressures (for example helpfulness vs. harmlessness). To understand how models resolve such conflicts, we study Llama-2-chat models on the forbidden fact task. Specifically, we instruct Llama-2 to truthfully complete a factual recall statement while forbidding it from saying the correct answer. This often makes the model give incorrect answers. We decompose Llama-2 into 1000+ components, and rank each one with respect to how useful it is for forbidding the correct answer. We find that in aggregate, around 35 components are enough to reliably implement the full suppression behavior. However, these components are fairly heterogeneous and many operate using faulty heuristics. We discover that one of these heuristics can be exploited via a manually designed adversarial attack which we call The California Attack. Our results highlight some roadblocks standing in the way of being able to successfully interpret advanced ML systems. Project website available at this https URL .",
    "urls": {
      "arxiv 2312": "http://arxiv.org/abs/2312.08793"
    }
  },
  {
    "id": 93,
    "title": "How do Language Models Bind Entities in Context?",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2024",
    "authors": [
      "Jiahai Feng",
      "Jacob Steinhardt"
    ],
    "abstract": "Language models (LMs) can recall facts mentioned in context, as shown by their performance on reading comprehension tasks. When the context describes facts about more than one entity, the LM has to correctly bind attributes to their corresponding entity. We show, via causal experiments, that LMs' internal activations represent binding information by exhibiting appropriate binding ID vectors at the entity and attribute positions. We further show that binding ID vectors form a subspace and often transfer across tasks. Our results demonstrate that LMs learn interpretable strategies for representing symbolic knowledge in context, and that studying context activations is a fruitful direction for understanding LM cognition.",
    "urls": {
      "ICLR 2024": "https://openreview.net/forum?id=zb3b6oKO77"
    }
  },
  {
    "id": 94,
    "title": "How Language Models Learn Context-Free Grammars?",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2023",
    "authors": [
      "Zeyuan Allen-Zhu",
      "Yuanzhi Li"
    ],
    "abstract": "We design experiments to study how generative language models, such as GPT, learn context-free grammars (CFGs) --- complex language systems with tree-like structures that encapsulate aspects of human logic, natural languages, and programs. CFGs, comparable in difficulty to pushdown automata, can be ambiguous, usually requiring dynamic programming for rule verification. We create synthetic data to show that pre-trained transformers can learn to generate sentences with near-perfect accuracy and impressive diversity, even for quite challenging CFGs. Crucially, we uncover the mechanisms behind transformers learning such CFGs. We find that the hidden states implicitly encode the CFG structure (such as putting tree node info exactly on the subtree boundary), and that the transformer can form \"boundary to boundary\" attentions that mimic dynamic programming. We also discuss CFG extensions and transformer robustness against grammar errors.",
    "urls": {
      "openreview": "https://openreview.net/forum?id=qnbLGV9oFL"
    }
  },
  {
    "id": 95,
    "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2024-01",
    "authors": [
      "Andrew Lee",
      "Xiaoyan Bai",
      "Itamar Pres",
      "Martin Wattenberg",
      "Jonathan K. Kummerfeld",
      "Rada Mihalcea"
    ],
    "abstract": "While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.",
    "urls": {
      "arxiv 2401": "http://arxiv.org/abs/2401.01967"
    }
  },
  {
    "id": 96,
    "title": "Do Llamas Work in English? On the Latent Language of Multilingual Transformers",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2024-02",
    "authors": [
      "Chris Wendler",
      "Veniamin Veselovsky",
      "Giovanni Monea",
      "Robert West"
    ],
    "abstract": "We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in \"input space\", \"concept space\", and \"output space\", respectively. Crucially, our evidence suggests that the abstract \"concept space\" lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.10588"
    }
  },
  {
    "id": 97,
    "title": "Evidence of Learned Look-Ahead in a Chess-Playing Neural Network",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2024-06",
    "authors": [
      "Erik Jenner",
      "Shreyas Kapur",
      "Vasil Georgiev",
      "Cameron Allen",
      "Scott Emmons",
      "Stuart Russell"
    ],
    "abstract": "Do neural networks learn to implement algorithms such as look-ahead or search \"in the wild\"? Or do they rely purely on collections of simple heuristics? We present evidence of learned look-ahead in the policy network of Leela Chess Zero, the currently strongest neural chess engine. We find that Leela internally represents future optimal moves and that these representations are crucial for its final output in certain board states. Concretely, we exploit the fact that Leela is a transformer that treats every chessboard square like a token in language models, and give three lines of evidence (1) activations on certain squares of future moves are unusually important causally; (2) we find attention heads that move important information \"forward and backward in time,\" e.g., from squares of future moves to squares of earlier ones; and (3) we train a simple probe that can predict the optimal move 2 turns ahead with 92% accuracy (in board states where Leela finds a single best line). These findings are an existence proof of learned look-ahead in neural networks and might be a step towards a better understanding of their capabilities.",
    "urls": {
      "arxiv2406": "https://arxiv.org/abs/2406.00877"
    }
  },
  {
    "id": 98,
    "title": "How much do contextualized representations encode long-range context?",
    "tags": [],
    "primaryTag": "Ability/Function",
    "date": "2024-10",
    "authors": [
      "Simeng Sun",
      "Cheng-Ping Hsieh"
    ],
    "abstract": "We analyze contextual representations in neural autoregressive language models, emphasizing long-range contexts that span several thousand tokens. Our methodology employs a perturbation setup and the metric \\emph{Anisotropy-Calibrated Cosine Similarity}, to capture the degree of contextualization of long-range patterns from the perspective of representation geometry. We begin the analysis with a case study on standard decoder-only Transformers, demonstrating that similar perplexity can exhibit markedly different downstream task performance, which can be explained by the difference in contextualization of long-range content. Next, we extend the analysis to other models, covering recent novel architectural designs and various training configurations. The representation-level results illustrate a reduced capacity for high-complexity (i.e., less compressible) sequences across architectures, and that fully recurrent models rely heavily on local context, whereas hybrid models more effectively encode the entire sequence structure. Finally, preliminary analysis of model size and training configurations on the encoding of long-range context suggest potential directions for improving existing language models.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.12292"
    }
  },
  {
    "id": 99,
    "title": "Progress measures for grokking via mechanistic interpretability",
    "tags": [],
    "primaryTag": "Ability/Arithmetic",
    "date": "",
    "authors": [],
    "abstract": "Anonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
    "urls": {
      "ICLR 2023": "https://openreview.net/forum?id=9XFSbDPmdW"
    }
  },
  {
    "id": 100,
    "title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
    "tags": [],
    "primaryTag": "Learning Dynamics/Phase Transition",
    "date": "2023",
    "authors": [
      "Bilal Chughtai",
      "Lawrence Chan",
      "Neel Nanda"
    ],
    "abstract": "Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small networks learn to implement group compositions. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that these networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks trained on various groups and architectures, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.",
    "urls": {
      "ICML 2023": "https://openreview.net/forum?id=jCOrkuUpss"
    }
  },
  {
    "id": 101,
    "title": "The Mechanistic Basis of Data Dependence and Abrupt Learning in an In-Context Classification Task",
    "tags": [],
    "primaryTag": "Learning Dynamics/Phase Transition",
    "date": "2024",
    "authors": [
      "Gautam Reddy"
    ],
    "abstract": "Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence, which contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.",
    "urls": {
      "ICLR 2024 oral": "https://openreview.net/forum?id=aN4Jf6Cx69"
    }
  },
  {
    "id": 102,
    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
    "tags": [],
    "primaryTag": "Learning Dynamics/Phase Transition",
    "date": "2024",
    "authors": [
      "Angelica Chen",
      "Ravid Shwartz-Ziv",
      "Kyunghyun Cho",
      "Matthew L Leavitt",
      "Naomi Saphra"
    ],
    "abstract": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
    "urls": {
      "ICLR 2024 spotlight": "https://openreview.net/forum?id=MO5PiKHELW"
    }
  },
  {
    "id": 103,
    "title": "A simple and interpretable model of grokking modular arithmetic tasks",
    "tags": [],
    "primaryTag": "Learning Dynamics/Phase Transition",
    "date": "2023",
    "authors": [
      "Andrey Gromov"
    ],
    "abstract": "We present a simple neural network that can generalize on various modular arithmetic tasks such as modular addition or multiplication, and exhibits a sudden jump in generalization known as \\emph{grokking}. Concretely, we present (i) fully-connected two-layer networks  that exhibit grokking on various modular arithmetic tasks under vanilla gradient descent with the MSE loss function in the absence of any regularization; (ii) evidence that grokking modular arithmetic corresponds to learning specific representations whose structure is determined by the task; (iii) \\emph{analytic} expressions for the weights -- and thus for the embedding -- that solve a large class of modular arithmetic tasks; and (iv) evidence that these representations are also found by gradient descent as well as AdamW, establishing complete (\"mechanistic\") interpretability of the representations learnt by the network.",
    "urls": {
      "openreview": "https://openreview.net/forum?id=0ZUKLCxwBo"
    }
  },
  {
    "id": 104,
    "title": "Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition",
    "tags": [],
    "primaryTag": "Learning Dynamics/Phase Transition",
    "date": "2024-02",
    "authors": [
      "Yufei Huang",
      "Shengding Hu",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Recent studies have uncovered intriguing phenomena in deep learning, such as grokking, double descent, and emergent abilities in large language models, which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive framework that provides a unified view of these three phenomena, focusing on the competition between memorization and generalization circuits. This approach, initially employed to explain grokking, is extended in our work to encompass a wider range of model sizes and training data volumes. Our framework delineates four distinct training dynamics, each depending on varying combinations of model size and training data quantity. Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and propose two verifiable predictions regarding its occurrence, both substantiated by our experimental results. Moreover, we expand our framework to the multi-task learning paradigm, demonstrating how algorithm tasks can be turned into emergent abilities. This offers a novel perspective to understand emergent abilities in Large Language Models.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.15175"
    }
  },
  {
    "id": 105,
    "title": "Interpreting Grokked Transformers in Complex Modular Arithmetic",
    "tags": [],
    "primaryTag": "Learning Dynamics/Phase Transition",
    "date": "2024-02",
    "authors": [
      "Hiroki Furuta",
      "Gouki Minegishi",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "abstract": "Grokking has been actively explored to reveal the mystery of delayed generalization and identifying interpretable representations and algorithms inside the grokked models is a suggestive hint to understanding its mechanism. Grokking on modular addition has been known to implement Fourier representation and its calculation circuits with trigonometric identities in Transformers. Considering the periodicity in modular arithmetic, the natural question is to what extent these explanations and interpretations hold for the grokking on other modular operations beyond addition. For a closer look, we first hypothesize that any modular operations can be characterized with distinctive Fourier representation or internal circuits, grokked models obtain common features transferable among similar operations, and mixing datasets with similar operations promotes grokking. Then, we extensively examine them by learning Transformers on complex modular arithmetic tasks, including polynomials. Our Fourier analysis and novel progress measure for modular arithmetic, Fourier Frequency Density and Fourier Coefficient Ratio, characterize distinctive internal representations of grokked models per modular operation; for instance, polynomials often result in the superposition of the Fourier components seen in elementary arithmetic, but clear patterns do not emerge in challenging non-factorizable polynomials. In contrast, our ablation study on the pre-grokked models reveals that the transferability among the models grokked with each operation can be only limited to specific combinations, such as from elementary arithmetic to linear expressions. Moreover, some multi-task mixtures may lead to co-grokking -- where grokking simultaneously happens for all the tasks -- and accelerate generalization, while others may not find optimal solutions. We provide empirical steps towards the interpretability of internal circuits.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.16726"
    }
  },
  {
    "id": 106,
    "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
    "tags": [],
    "primaryTag": "Learning Dynamics/Phase Transition",
    "date": "2024-02",
    "authors": [
      "Chen Qian",
      "Jie Zhang",
      "Wei Yao",
      "Dongrui Liu",
      "Zhenfei Yin",
      "Yu Qiao",
      "Yong Liu",
      "Jing Shao"
    ],
    "abstract": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \\textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \\url{this https URL}.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.19465"
    }
  },
  {
    "id": 107,
    "title": "Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks",
    "tags": [],
    "primaryTag": "Learning Dynamics/Phase Transition",
    "date": "2024-06",
    "authors": [
      "Tianyu He",
      "Darshil Doshi",
      "Aritra Das",
      "Andrey Gromov"
    ],
    "abstract": "Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions z=ax+bymodpz = a \\, x + b \\, y \\;\\mathrm{mod}\\; p labeled by the vector (a,b)∈ℤ2p(a, b) \\in \\mathbb{Z}_p^2. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is \\emph{transient}, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing highly structured representations in both attention heads and MLPs; and discuss the learned algorithms. Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples.",
    "urls": {
      "arxiv 2406": "https://arxiv.org/abs/2406.02550"
    }
  },
  {
    "id": 108,
    "title": "Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization",
    "tags": [],
    "primaryTag": "Learning Dynamics/Phase Transition",
    "date": "2024",
    "authors": [
      "Boshi Wang",
      "Xiang Yue",
      "Yu Su",
      "Huan Sun"
    ],
    "abstract": "Submission Number: 44",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=ns8IH5Sn5y"
    }
  },
  {
    "id": 109,
    "title": "Studying Large Language Model Generalization with Influence Functions",
    "tags": [],
    "primaryTag": "Learning Dynamics/Fine-tuning",
    "date": "2023-08",
    "authors": [
      "Roger Grosse",
      "Juhan Bae",
      "Cem Anil",
      "Nelson Elhage",
      "Alex Tamkin",
      "Amirhossein Tajdini",
      "Benoit Steiner",
      "Dustin Li",
      "Esin Durmus",
      "Ethan Perez",
      "Evan Hubinger",
      "Kamilė Lukošiūtė",
      "Karina Nguyen",
      "Nicholas Joseph",
      "Sam McCandlish",
      "Jared Kaplan",
      "Samuel R. Bowman"
    ],
    "abstract": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.",
    "urls": {
      "arxiv 2308": "http://arxiv.org/abs/2308.03296"
    }
  },
  {
    "id": 110,
    "title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
    "tags": [],
    "primaryTag": "Learning Dynamics/Fine-tuning",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "ICLR 2024": "https://openreview.net/forum?id=A0HKeKl4Nl"
    }
  },
  {
    "id": 111,
    "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking",
    "tags": [],
    "primaryTag": "Learning Dynamics/Fine-tuning",
    "date": "2024",
    "authors": [
      "Nikhil Prakash",
      "Tamar Rott Shaham",
      "Tal Haklay",
      "Yonatan Belinkov",
      "David Bau"
    ],
    "abstract": "Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify a mechanism that enables entity tracking and show that (i) both the original model and its fine-tuned version implement entity tracking with the same circuit. In fact, the entity tracking circuit of the fine-tuned version performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionality, that is entity tracking is performed by tracking the position of the correct entity in both the original model and its fine-tuned version. (iii) Performance boost in the fine-tuned model is primarily attributed to its improved ability to handle positional information. To uncover these findings, we employ two methods: DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms. Our findings suggest that fine-tuning enhances, rather than fundamentally alters, the mechanistic operation of the model.",
    "urls": {
      "ICLR 2024": "https://openreview.net/forum?id=8sKcAWOf2D"
    }
  },
  {
    "id": 112,
    "title": "The Hidden Space of Transformer Language Adapters",
    "tags": [],
    "primaryTag": "Learning Dynamics/Fine-tuning",
    "date": "2024-02",
    "authors": [
      "Jesujoba O. Alabi",
      "Marius Mosbach",
      "Matan Eyal",
      "Dietrich Klakow",
      "Mor Geva"
    ],
    "abstract": "We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.13137"
    }
  },
  {
    "id": 113,
    "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
    "tags": [],
    "primaryTag": "Learning Dynamics/Fine-tuning",
    "date": "2024-10",
    "authors": [
      "Yihuai Hong",
      "Yuelin Zou",
      "Lijie Hu",
      "Ziqian Zeng",
      "Di Wang",
      "Haiqin Yang"
    ],
    "abstract": "Fine-tuning-based unlearning methods prevail for preventing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of these methods is unclear. In this work, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model's knowledge retrieval process, providing further evidence that they do not genuinely erase the problematic knowledge embedded in the model parameters. Instead, the coefficients generated by the MLP components in the model's final layer are the primary contributors to these seemingly positive unlearning effects, playing a crucial role in controlling the model's behaviors. Furthermore, behavioral tests demonstrate that this unlearning mechanism inevitably impacts the global behavior of the models, affecting unrelated knowledge or capabilities. The code is released at this https URL.",
    "urls": {
      "EMNLP 2024": "https://arxiv.org/abs/2410.06606"
    }
  },
  {
    "id": 114,
    "title": "Implicit Representations of Meaning in Neural Language Models",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "ACL 2021": "https://aclanthology.org/2021.acl-long.143"
    }
  },
  {
    "id": 115,
    "title": "All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2023-05",
    "authors": [
      "Yuxin Ren",
      "Qipeng Guo",
      "Zhijing Jin",
      "Shauli Ravfogel",
      "Mrinmaya Sachan",
      "Bernhard Schölkopf",
      "Ryan Cotterell"
    ],
    "abstract": "Transformer models bring propelling advances in various NLP tasks, thus inducing lots of interpretability research on the learned representations of the models. However, we raise a fundamental question regarding the reliability of the representations. Specifically, we investigate whether transformers learn essentially isomorphic representation spaces, or those that are sensitive to the random seeds in their pretraining process. In this work, we formulate the Bijection Hypothesis, which suggests the use of bijective methods to align different models' representation spaces. We propose a model based on invertible neural networks, BERT-INN, to learn the bijection more effectively than other existing bijective methods such as the canonical correlation analysis (CCA). We show the advantage of BERT-INN both theoretically and through extensive experiments, and apply it to align the reproduced BERT embeddings to draw insights that are meaningful to the interpretability research. Our code is at this https URL.",
    "urls": {
      "arxiv 2305": "http://arxiv.org/abs/2305.14555"
    }
  },
  {
    "id": 116,
    "title": "Observable Propagation: Uncovering Feature Vectors in Transformers",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2023",
    "authors": [
      "Jacob Dunefsky",
      "Arman Cohan"
    ],
    "abstract": "A key goal of current mechanistic interpretability research in NLP is to find linear features (also called \"feature vectors\") for transformers:  directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data -- both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called \"observable propagation\" (in short: \"ObsProp\"), for finding linear features used by transformer language models in computing a given task -- using almost no data. Our paradigm centers on the concept of \"observables\", linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors: we prove that LayerNorm nonlinearities in high dimensions do not affect the direction of feature vectors; we also introduce a similarity metric between feature vectors called the \"coupling coefficient\", and prove that it accurately estimates the degree to which one feature's output correlates with another's. Armed with these tools, we use observable propagation to investigate the features that cause gendered occupational bias in a large language model. In our experiments, we identify the specific features used by the model for predicting occupation based on a gendered name, and find that some of the same features are used by the model for predicting grammatical gender. Our results suggest that observable propagation can be used to better understand the mechanisms responsible for bias in large language models.",
    "urls": {
      "openreview": "https://openreview.net/forum?id=sNWQUTkDmA"
    }
  },
  {
    "id": 117,
    "title": "In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2023",
    "authors": [
      "Safoora Yousefi",
      "Hosein Hasanbeig",
      "Leo Moreno Betthauser",
      "Akanksha Saran",
      "Raphaël Millière",
      "Ida Momennejad"
    ],
    "abstract": "Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate how LLM embeddings and attention representations change following in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques such as representational similarity analysis (RSA) and propose novel methods for parameterized probing and measuring ratio of attention to relevant vs. irrelevant information in Llama-2 70B and Vicuna 13B. We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attention. Our analyses revealed a meaningful correlation between changes in both embeddings and attention representations with improvements in behavioral performance after ICL. This empirical framework empowers a nuanced understanding of how latent representations affect LLM behavior with and without ICL, offering valuable tools and insights for future research and practical applications.",
    "urls": {
      "openreview": "https://openreview.net/forum?id=UEdS2lIgfY"
    }
  },
  {
    "id": 118,
    "title": "Challenges with unsupervised LLM knowledge discovery",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2023-12",
    "authors": [
      "Sebastian Farquhar",
      "Vikrant Varma",
      "Zachary Kenton",
      "Johannes Gasteiger",
      "Vladimir Mikulik",
      "Rohin Shah"
    ],
    "abstract": "We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent. The idea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure, which can be used to discover knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method, contrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a series of experiments showing settings in which unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. Conceptually, we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods.",
    "urls": {
      "arxiv 2312": "https://arxiv.org/abs/2312.10029"
    }
  },
  {
    "id": 119,
    "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2023-06",
    "authors": [
      "B.A. Levinstein",
      "Daniel A. Herrmann"
    ],
    "abstract": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",
    "urls": {
      "arxiv 2307": "http://arxiv.org/abs/2307.00175"
    }
  },
  {
    "id": 120,
    "title": "Position Paper: Toward New Frameworks for Studying Model Representations",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024-02",
    "authors": [
      "Satvik Golechha",
      "James Dao"
    ],
    "abstract": "Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities important for safety and trust are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We formalize representations for features and behaviors, highlight their importance and evaluation, and perform an exploratory study of dishonesty representations in `Mistral-7B-Instruct-v0.1'. We justify that studying representations is an important and under-studied field, and highlight several challenges that arise while attempting to do so through currently established methods in MI, showing their insufficiency and advocating work on new frameworks for the same.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.03855"
    }
  },
  {
    "id": 121,
    "title": "How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024-02",
    "authors": [
      "Tianjie Ju",
      "Weiwei Sun",
      "Wei Du",
      "Xinwei Yuan",
      "Zhaochun Ren",
      "Gongshen Liu"
    ],
    "abstract": "Previous work has showcased the intriguing capability of large language models (LLMs) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of LLMs through probing tasks. We leverage the powerful generative capability of ChatGPT to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ \\mathcal V-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens at lower layers while progressively expanding more knowledge within other tokens at upper layers; and (3) gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence. Code is publicly available at this https URL.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.16061"
    }
  },
  {
    "id": 122,
    "title": "More than Correlation: Do Large Language Models Learn Causal Representations of Space",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2023-12",
    "authors": [
      "Yida Chen",
      "Yixian Gan",
      "Sijia Li",
      "Li Yao",
      "Xiaohan Zhao"
    ],
    "abstract": "Recent work found high mutual information between the learned representations of large language models (LLMs) and the geospatial property of its input, hinting an emergent internal model of space. However, whether this internal space model has any causal effects on the LLMs' behaviors was not answered by that work, led to criticism of these findings as mere statistical correlation. Our study focused on uncovering the causality of the spatial representations in LLMs. In particular, we discovered the potential spatial representations in DeBERTa, GPT-Neo using representational similarity analysis and linear and non-linear probing. Our casual intervention experiments showed that the spatial representations influenced the model's performance on next word prediction and a downstream task that relies on geospatial information. Our experiments suggested that the LLMs learn and use an internal model of space in solving geospatial related tasks.",
    "urls": {
      "arxiv 2312": "https://arxiv.org/abs/2312.16257"
    }
  },
  {
    "id": 123,
    "title": "Do Large Language Models Mirror Cognitive Language Processing?",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024-02",
    "authors": [
      "Yuqi Ren",
      "Renren Jin",
      "Tongxuan Zhang",
      "Deyi Xiong"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in text comprehension and logical reasoning, indicating that the text representations learned by LLMs can facilitate their language processing capabilities. In neuroscience, brain cognitive processing signals are typically utilized to study human language processing. Therefore, it is natural to ask how well the text embeddings from LLMs align with the brain cognitive processing signals, and how training strategies affect the LLM-brain alignment? In this paper, we employ Representational Similarity Analysis (RSA) to measure the alignment between 23 mainstream LLMs and fMRI signals of the brain to evaluate how effectively LLMs simulate cognitive language processing. We empirically investigate the impact of various factors (e.g., pre-training data size, model scaling, alignment training, and prompts) on such LLM-brain alignment. Experimental results indicate that pre-training data size and model scaling are positively correlated with LLM-brain similarity, and alignment training can significantly improve LLM-brain similarity. Explicit prompts contribute to the consistency of LLMs with brain cognitive language processing, while nonsensical noisy prompts may attenuate such alignment. Additionally, the performance of a wide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated with the LLM-brain similarity.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.18023"
    }
  },
  {
    "id": 124,
    "title": "On the Scaling Laws of Geographical Representation in Language Models",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024-02",
    "authors": [
      "Nathan Godey",
      "Éric de la Clergerie",
      "Benoît Sagot"
    ],
    "abstract": "Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to Large Language Models (LLMs). In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.19406"
    }
  },
  {
    "id": 125,
    "title": "Monotonic Representation of Numeric Properties in Language Models",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024-03",
    "authors": [
      "Benjamin Heinzerling",
      "Kentaro Inui"
    ],
    "abstract": "Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model's internal representations is not understood well. Here, we introduce a simple method for finding and editing representations of numeric properties such as an entity's birth year. Empirically, we find low-dimensional subspaces that encode numeric properties monotonically, in an interpretable and editable fashion. When editing representations along directions in these subspaces, LM output changes accordingly. For example, by patching activations along a \"birthyear\" direction we can make the LM express an increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was born in 1957, Karl Popper was born in 1968. Property-encoding directions exist across several numeric properties in all models under consideration, suggesting the possibility that monotonic representation of numeric properties consistently emerges during LM pretraining. Code: this https URL",
    "urls": {
      "arxiv 2403": "http://arxiv.org/abs/2403.10381"
    }
  },
  {
    "id": 126,
    "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024-04",
    "authors": [
      "Mingyu Jin",
      "Qinkai Yu",
      "Jingyuan Huang",
      "Qingcheng Zeng",
      "Zhenting Wang",
      "Wenyue Hua",
      "Haiyan Zhao",
      "Kai Mei",
      "Yanda Meng",
      "Kaize Ding",
      "Fan Yang",
      "Mengnan Du",
      "Yongfeng Zhang"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable performances across a wide range of tasks. However, the mechanisms by which these models encode tasks of varying complexities remain poorly understood. In this paper, we explore the hypothesis that LLMs process concepts of varying complexities in different layers, introducing the idea of \"Concept Depth\" to suggest that more complex concepts are typically acquired in deeper layers. Specifically, we categorize concepts based on their level of abstraction, defining them in the order of increasing complexity within factual, emotional, and inferential tasks. We conduct extensive probing experiments using layer-wise representations across various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the three domains of tasks. Our findings reveal that models could efficiently conduct probing for simpler tasks in shallow layers, and more complex tasks typically necessitate deeper layers for accurate understanding. Additionally, we examine how external factors, such as adding noise to the input and quantizing the model weights, might affect layer-wise representations. Our findings suggest that these factors can impede the development of a conceptual understanding of LLMs until deeper layers are explored. We hope that our proposed concept and experimental insights will enhance the understanding of the mechanisms underlying LLMs. Our codes are available at this https URL.",
    "urls": {
      "arxiv 2404": "http://arxiv.org/abs/2404.07066"
    }
  },
  {
    "id": 127,
    "title": "Simple probes can catch sleeper agents",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Anthropic Blog": "https://www.anthropic.com/research/probes-catch-sleeper-agents"
    }
  },
  {
    "id": 128,
    "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024-06",
    "authors": [
      "Jinqi Luo",
      "Tianjiao Ding",
      "Kwan Ho Ryan Chan",
      "Darshan Thaker",
      "Aditya Chattopadhyay",
      "Chris Callison-Burch",
      "René Vidal"
    ],
    "abstract": "Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable outputs via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Then, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activations as linear combinations of benign and undesirable components. By removing the latter ones from the activations, we reorient the behavior of the LLM towards the alignment goal. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.",
    "urls": {
      "arxiv 2406": "https://arxiv.org/abs/2406.04331"
    }
  },
  {
    "id": 129,
    "title": "The Geometry of Categorical and Hierarchical Concepts in Large Language Models",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024",
    "authors": [
      "Kiho Park",
      "Yo Joong Choe",
      "Yibo Jiang",
      "Victor Veitch"
    ],
    "abstract": "Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability. In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as {'mammal', 'bird', 'reptile', 'fish'}, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that 'dog' is a kind of 'mammal' encoded? We show how to extend the linear representation hypothesis to answer these questions. We then find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure. We validate the results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from the WordNet hierarchy.",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=KXuYjuBzKo"
    }
  },
  {
    "id": 130,
    "title": "Concept Space Alignment in Multilingual LLMs",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024-10",
    "authors": [
      "Qiwei Peng",
      "Anders Søgaard"
    ],
    "abstract": "Multilingual large language models (LLMs) seem to generalize somewhat across languages. We hypothesize this is a result of implicit vector space alignment. Evaluating such alignment, we see that larger models exhibit very high-quality linear alignments between corresponding concepts in different languages. Our experiments show that multilingual LLMs suffer from two familiar weaknesses: generalization works best for languages with similar typology, and for abstract concepts. For some models, e.g., the Llama-2 family of models, prompt-based embeddings align better than word embeddings, but the projections are less linear -- an observation that holds across almost all model families, indicating that some of the implicitly learned alignments are broken somewhat by prompt-based methods.",
    "urls": {
      "EMNLP 2024": "https://arxiv.org/abs/2410.01079"
    }
  },
  {
    "id": 131,
    "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models",
    "tags": [],
    "primaryTag": "Representation/General",
    "date": "2024-10",
    "authors": [
      "Michael Lan",
      "Philip Torr",
      "Austin Meek",
      "Ashkan Khakzar",
      "David Krueger",
      "Fazl Barez"
    ],
    "abstract": "We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones, making it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics on SAE feature spaces across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.06981"
    }
  },
  {
    "id": 132,
    "title": "Actually, Othello-GPT Has A Linear Emergent World Representation",
    "tags": [],
    "primaryTag": "Representation/Linearity",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Neel Nanda's blog": "https://www.neelnanda.io/mechanistic-interpretability/othello"
    }
  },
  {
    "id": 133,
    "title": "Language Models Linearly Represent Sentiment",
    "tags": [],
    "primaryTag": "Representation/Linearity",
    "date": "2023",
    "authors": [
      "Curt Tigges",
      "Oskar John Hollinsworth",
      "Neel Nanda",
      "Atticus Geiger"
    ],
    "abstract": "Sentiment is a pervasive feature in natural language text, yet it is an open question how sentiment is represented within Large Language Models (LLMs). In this study, we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative. Through causal interventions, we isolate this direction and show it is causally relevant in both toy tasks and real world datasets such as Stanford Sentiment Treebank. \nWe further uncover the mechanisms that involve this direction, highlighting the roles of a small subset of attention heads and neurons. Finally, we discover a phenomenon which we term the summarization motif: sentiment is not solely represented on emotionally charged words, but is additionally summarised at intermediate positions without inherent sentiment, such as punctuation and names. We show that in Stanford Sentiment Treebank zero-shot classification, 76% of above-chance classification accuracy is lost when ablating the sentiment direction, nearly half of which (36%) is due to ablating the summarized sentiment direction exclusively at comma positions.",
    "urls": {
      "openreview": "https://openreview.net/forum?id=iGDWZFc7Ya"
    }
  },
  {
    "id": 134,
    "title": "Language Models Represent Space and Time",
    "tags": [],
    "primaryTag": "Representation/Linearity",
    "date": "2024",
    "authors": [
      "Wes Gurnee",
      "Max Tegmark"
    ],
    "abstract": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.",
    "urls": {
      "openreview": "https://openreview.net/forum?id=jE8xbmvFin"
    }
  },
  {
    "id": 135,
    "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets",
    "tags": [],
    "primaryTag": "Representation/Linearity",
    "date": "2023",
    "authors": [
      "Samuel Marks",
      "Max Tegmark"
    ],
    "abstract": "Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we curate high-quality datasets of true/false statements and use them to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgicallly intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that language models linearly represent the truth or falsehood of factual statements. We also introduce a novel technique, mass-mean probing, which generalizes better and is more causally implicated in model outputs than other probing techniques.",
    "urls": {
      "openreview": "https://openreview.net/forum?id=CeJEfNKstt"
    }
  },
  {
    "id": 136,
    "title": "Linearity of Relation Decoding in Transformer Language Models",
    "tags": [],
    "primaryTag": "Representation/Linearity",
    "date": "2024",
    "authors": [
      "Evan Hernandez",
      "Arnab Sen Sharma",
      "Tal Haklay",
      "Kevin Meng",
      "Martin Wattenberg",
      "Jacob Andreas",
      "Yonatan Belinkov",
      "David Bau"
    ],
    "abstract": "Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.",
    "urls": {
      "ICLR 2024": "https://openreview.net/forum?id=w7LU2s14kE"
    }
  },
  {
    "id": 137,
    "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models",
    "tags": [],
    "primaryTag": "Representation/Linearity",
    "date": "2023-11",
    "authors": [
      "Kiho Park",
      "Yo Joong Choe",
      "Victor Veitch"
    ],
    "abstract": "Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does \"linear representation\" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of \"linear representation\", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.",
    "urls": {
      "arxiv 2311": "https://arxiv.org/abs/2311.03658"
    }
  },
  {
    "id": 138,
    "title": "Language Models Represent Beliefs of Self and Others",
    "tags": [],
    "primaryTag": "Representation/Linearity",
    "date": "2024-02",
    "authors": [
      "Wentao Zhu",
      "Zhining Zhang",
      "Yizhou Wang"
    ],
    "abstract": "Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.18496"
    }
  },
  {
    "id": 139,
    "title": "On the Origins of Linear Representations in Large Language Models",
    "tags": [],
    "primaryTag": "Representation/Linearity",
    "date": "2024-03",
    "authors": [
      "Yibo Jiang",
      "Goutham Rajendran",
      "Pradeep Ravikumar",
      "Bryon Aragam",
      "Victor Veitch"
    ],
    "abstract": "Recent works have argued that high-level semantic concepts are encoded \"linearly\" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.",
    "urls": {
      "arxiv 2403": "http://arxiv.org/abs/2403.03867"
    }
  },
  {
    "id": 140,
    "title": "Refusal in LLMs is mediated by a single direction",
    "tags": [],
    "primaryTag": "Representation/Linearity",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "Lesswrong 2024": "https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction"
    }
  },
  {
    "id": 141,
    "title": "Aligning Large Language Models with Human Preferences through Representation Engineering",
    "tags": [],
    "primaryTag": "Application/Training",
    "date": "2023-12",
    "authors": [
      "Wenhao Liu",
      "Xiaohua Wang",
      "Muling Wu",
      "Tianlong Li",
      "Changze Lv",
      "Zixuan Ling",
      "Jianhao Zhu",
      "Cenyuan Zhang",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ],
    "abstract": "Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involves employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in this http URL inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM, and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to this http URL experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance.",
    "urls": {
      "arxiv2312": "http://arxiv.org/abs/2312.15997"
    }
  },
  {
    "id": 142,
    "title": "ReFT: Representation Finetuning for Language Models",
    "tags": [],
    "primaryTag": "Application/Training",
    "date": "2024-04",
    "authors": [
      "Zhengxuan Wu",
      "Aryaman Arora",
      "Zheng Wang",
      "Atticus Geiger",
      "Dan Jurafsky",
      "Christopher D. Manning",
      "Christopher Potts"
    ],
    "abstract": "Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15x--65x more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly at this https URL.",
    "urls": {
      "arxiv 2404": "https://arxiv.org/abs/2404.03592",
      "github": "https://github.com/stanfordnlp/pyreft"
    }
  },
  {
    "id": 143,
    "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints",
    "tags": [],
    "primaryTag": "Application/Training",
    "date": "2024-11",
    "authors": [
      "Qingyu Yin",
      "Chak Tou Leong",
      "Hongbo Zhang",
      "Minjun Zhu",
      "Hanqi Yan",
      "Qiang Zhang",
      "Yulan He",
      "Wenjie Li",
      "Jun Wang",
      "Yue Zhang",
      "Linyi Yang"
    ],
    "abstract": "The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.",
    "urls": {
      "arxiv2411": "https://arxiv.org/abs/2411.07618"
    }
  },
  {
    "id": 144,
    "title": "LLM Pretraining with Continuous Concepts",
    "tags": [],
    "primaryTag": "Application/Training",
    "date": "2025-02",
    "authors": [
      "Jihoon Tack",
      "Jack Lanchantin",
      "Jane Yu",
      "Andrew Cohen",
      "Ilia Kulikov",
      "Janice Lan",
      "Shibo Hao",
      "Yuandong Tian",
      "Jason Weston",
      "Xian Li"
    ],
    "abstract": "Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process.",
    "urls": {
      "arxiv2502": "https://arxiv.org/abs/2502.08524"
    }
  },
  {
    "id": 145,
    "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2023",
    "authors": [
      "Kenneth Li",
      "Oam Patel",
      "Fernanda Viégas",
      "Hanspeter Pfister",
      "Martin Wattenberg"
    ],
    "abstract": "We introduce Inference-Time Intervention (ITI), a technique designed to enhance the \"truthfulness\" of large language models (LLMs). ITI operates by shifting model activations during inference, following a learned set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5%32.5%32.5\\% to 65.1%65.1%65.1\\%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.",
    "urls": {
      "NIPS 2023": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html",
      "github": "https://github.com/likenneth/honest_llama"
    }
  },
  {
    "id": 146,
    "title": "Activation Addition: Steering Language Models Without Optimization",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2023-08",
    "authors": [
      "Alexander Matt Turner",
      "Lisa Thiergart",
      "Gavin Leech",
      "David Udell",
      "Juan J. Vazquez",
      "Ulisse Mini",
      "Monte MacDiarmid"
    ],
    "abstract": "Prompt engineering and finetuning aim to maximize language model performance on a given metric (like toxicity reduction). However, these methods do not fully elicit a model's capabilities. To reduce this gap, we introduce activation engineering: the inference-time modification of activations in order to control (or steer) model outputs. Specifically, we introduce the Activation Addition (ActAdd) technique, which contrasts the intermediate activations on prompt pairs (such as \"Love\" versus \"Hate\") to compute a steering vector (Subramani et al. 2022). By tactically adding in e.g. the \"Love\" - \"Hate\" steering vector during the forward pass, we achieve SOTA on negative-to-positive sentiment shift and detoxification using models including LLaMA-3 and OPT. ActAdd yields inference-time control over high-level output properties (like topic and sentiment) while preserving performance on off-target tasks. ActAdd is lightweight: it does not require any machine optimization and works with a single pair of data points, which enables rapid iteration over steering. ActAdd demonstrates the power of activation engineering.",
    "urls": {
      "arxiv 2308": "http://arxiv.org/abs/2308.10248"
    }
  },
  {
    "id": 147,
    "title": "Self-Detoxifying Language Models via Toxification Reversal",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "EMNLP 2023": "https://aclanthology.org/2023.emnlp-main.269"
    }
  },
  {
    "id": 148,
    "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2023-09",
    "authors": [
      "Yung-Sung Chuang",
      "Yujia Xie",
      "Hongyin Luo",
      "Yoon Kim",
      "James Glass",
      "Pengcheng He"
    ],
    "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.",
    "urls": {
      "arxiv 2309": "https://arxiv.org/abs/2309.03883"
    }
  },
  {
    "id": 149,
    "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2023-11",
    "authors": [
      "Sheng Liu",
      "Haotian Ye",
      "Lei Xing",
      "James Zou"
    ],
    "abstract": "Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs.",
    "urls": {
      "arxiv 2311": "http://arxiv.org/abs/2311.06668"
    }
  },
  {
    "id": 150,
    "title": "Steering Llama 2 via Contrastive Activation Addition",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2023-12",
    "authors": [
      "Nina Panickssery",
      "Nick Gabrieli",
      "Julian Schulz",
      "Meg Tong",
      "Evan Hubinger",
      "Alexander Matt Turner"
    ],
    "abstract": "We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes \"steering vectors\" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).",
    "urls": {
      "arxiv 2312": "http://arxiv.org/abs/2312.06681"
    }
  },
  {
    "id": 151,
    "title": "A Language Model's Guide Through Latent Space",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2024-02",
    "authors": [
      "Dimitri von Rütte",
      "Sotiris Anagnostidis",
      "Gregor Bachmann",
      "Thomas Hofmann"
    ],
    "abstract": "Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time. While the focus of previous work has largely been on truthfulness, in this paper we extend this framework to a richer set of concepts such as appropriateness, humor, creativity and quality, and explore to what degree current detection and guidance strategies work in these challenging settings. To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model. Our extensive experiments reveal that while some concepts such as truthfulness more easily allow for guidance with current techniques, novel concepts such as appropriateness or humor either remain difficult to elicit, need extensive tuning to work, or even experience confusion. Moreover, we find that probes with optimal detection accuracies do not necessarily make for the optimal guides, contradicting previous observations for truthfulness. Our work warrants a deeper investigation into the interplay between detectability, guidability, and the nature of the concept, and we hope that our rich experimental test-bed for guidance research inspires stronger follow-up approaches.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.14433"
    }
  },
  {
    "id": 152,
    "title": "Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2023-11",
    "authors": [
      "Haoran Wang",
      "Kai Shu"
    ],
    "abstract": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated commendable results on various safety benchmarks, the vulnerability of their safety alignment has not been extensively studied. This is particularly troubling given the potential harm that LLMs can inflict. Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts. These approaches compromise the stealthiness and generalizability of the attacks, making them susceptible to detection. Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications. In this work, we study a different attack scenario, called Trojan Activation Attack (TA^2), which injects trojan steering vectors into the activation layers of LLMs. These malicious steering vectors can be triggered at inference time to steer the models toward attacker-desired behaviors by manipulating their activations. Our experiment results on four primary alignment tasks show that TA^2 is highly effective and adds little or no overhead to attack efficiency. Additionally, we discuss potential countermeasures against such activation attacks.",
    "urls": {
      "arxiv 2311": "https://arxiv.org/abs/2311.09433"
    }
  },
  {
    "id": 153,
    "title": "Extending Activation Steering to Broad Skills and Multiple Behaviours",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2024-03",
    "authors": [
      "Teun van der Weij",
      "Massimo Poesio",
      "Nandi Schoots"
    ],
    "abstract": "Current large language models have dangerous capabilities, which are likely to become more problematic in the future. Activation steering techniques can be used to reduce risks from these capabilities. In this paper, we investigate the efficacy of activation steering for broad skills and multiple behaviours. First, by comparing the effects of reducing performance on general coding ability and Python-specific ability, we find that steering broader skills is competitive to steering narrower skills. Second, we steer models to become more or less myopic and wealth-seeking, among other behaviours. In our experiments, combining steering vectors for multiple different behaviours into one steering vector is largely unsuccessful. On the other hand, injecting individual steering vectors at different places in a model simultaneously is promising.",
    "urls": {
      "arxiv 2403": "https://arxiv.org/abs/2403.05767"
    }
  },
  {
    "id": 154,
    "title": "Spectral Editing of Activations for Large Language Model Alignment",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2024-05",
    "authors": [
      "Yifu Qiu",
      "Zheng Zhao",
      "Yftah Ziser",
      "Anna Korhonen",
      "Edoardo M. Ponti",
      "Shay B. Cohen"
    ],
    "abstract": "Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content. Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods. We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated). We also extend our method to non-linear editing using feature functions. We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families. The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as computation and data efficiency. We also show that SEA editing only has a limited negative impact on other model capabilities.",
    "urls": {
      "arxiv 2405": "http://arxiv.org/abs/2405.09719"
    }
  },
  {
    "id": 155,
    "title": "Controlling Large Language Model Agents with Entropic Activation Steering",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2024-05",
    "authors": [
      "Tianlong Wang",
      "Xianfeng Jiao",
      "Yinghao Zhu",
      "Zhongzhi Chen",
      "Yifan He",
      "Xu Chu",
      "Junyi Gao",
      "Yasha Wang",
      "Liantao Ma"
    ],
    "abstract": "Recent studies have indicated that Large Language Models (LLMs) harbor an inherent understanding of truthfulness, yet often fail to consistently express it and generate false statements. This gap between \"knowing\" and \"telling\" poses a challenge for ensuring the truthfulness of generated content. Inspired by recent work on the practice of encoding human-interpretable concepts linearly within large language models, we treat truthfulness as a specially linearly encoded concept within LLMs, and introduce Adaptive Activation Steering (ACT), a tuning-free method that adaptively shifts LLM's activations in the \"truthful\" direction during inference. ACT addresses diverse categories of hallucinations by utilizing diverse truthfulness-related steering vectors and adjusting the steering intensity adaptively. Applied as an add-on across various models, ACT significantly improves truthfulness in LLaMA (↑\\uparrow 142%), LLaMA2 (↑\\uparrow 24%), Alpaca (↑\\uparrow 36%), Vicuna (↑\\uparrow 28%), LLaMA2-Chat (↑\\uparrow 19%), and LLaMA3(↑\\uparrow 34%). Furthermore, we verify ACT's scalability across larger models (13B, 33B, 65B), underscoring the adaptability of ACT to large-scale language models. Our code is available at this https URL.",
    "urls": {
      "arxiv 2406": "https://arxiv.org/abs/2406.00034"
    }
  },
  {
    "id": 156,
    "title": "Analyzing the Generalization and Reliability of Steering Vectors",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2024",
    "authors": [
      "Daniel Chee Hian Tan",
      "David Chanin",
      "Aengus Lynch",
      "Adrià Garriga-Alonso",
      "Dimitrios Kanoulas",
      "Brooks Paige",
      "Robert Kirk"
    ],
    "abstract": "Steering vectors (SVs) are a new approach to efficiently adjust language model behaviour at inference time by intervening on intermediate model activations. They have shown promise in terms of improving both capabilities and model alignment. However, the reliability and generalisation properties of this approach are unknown. In this work, we rigorously investigate these properties, and show that steering vectors have substantial limitations both in- and out-of-distribution. In-distribution, steerability is highly variable across different inputs. Depending on the concept, spurious biases can substantially contribute to how effective steering is for each input, presenting a challenge for the widespread use of steering vectors. Out-of-distribution, while steering vectors often generalise well, for several concepts they are brittle to reasonable changes in the prompt, resulting in them failing to generalise well. Overall, our findings show that while steering can work well in the right circumstances, there remain many technical difficulties of applying steering vectors to guide models' behaviour at scale.",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=akCsMk4dDL"
    }
  },
  {
    "id": 157,
    "title": "Towards Inference-time Category-wise Safety Steering for Large Language Models",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2024-10",
    "authors": [
      "Amrita Bhattacharjee",
      "Shaona Ghosh",
      "Traian Rebedea",
      "Christopher Parisien"
    ],
    "abstract": "While large language models (LLMs) have seen unprecedented advancements in capabilities and applications across a variety of use-cases, safety alignment of these models is still an area of active research. The fragile nature of LLMs, even models that have undergone extensive alignment and safety training regimes, warrants additional safety steering steps via training-free, inference-time methods. While recent work in the area of mechanistic interpretability has investigated how activations in latent representation spaces may encode concepts, and thereafter performed representation engineering to induce such concepts in LLM outputs, the applicability of such for safety is relatively under-explored. Unlike recent inference-time safety steering works, in this paper we explore safety steering of LLM outputs using: (i) category-specific steering vectors, thereby enabling fine-grained control over the steering, and (ii) sophisticated methods for extracting informative steering vectors for more effective safety steering while retaining quality of the generated text. We demonstrate our exploration on multiple LLMs and datasets, and showcase the effectiveness of the proposed steering method, along with a discussion on the implications and best practices.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.01174"
    }
  },
  {
    "id": 158,
    "title": "A Timeline and Analysis for Representation Plasticity in Large Language Models",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2024-10",
    "authors": [
      "Akshat Kannan"
    ],
    "abstract": "The ability to steer AI behavior is crucial to preventing its long term dangerous and catastrophic potential. Representation Engineering (RepE) has emerged as a novel, powerful method to steer internal model behaviors, such as \"honesty\", at a top-down level. Understanding the steering of representations should thus be placed at the forefront of alignment initiatives. Unfortunately, current efforts to understand plasticity at this level are highly neglected. This paper aims to bridge the knowledge gap and understand how LLM representation stability, specifically for the concept of \"honesty\", and model plasticity evolve by applying steering vectors extracted at different fine-tuning stages, revealing differing magnitudes of shifts in model behavior. The findings are pivotal, showing that while early steering exhibits high plasticity, later stages have a surprisingly responsive critical window. This pattern is observed across different model architectures, signaling that there is a general pattern of model plasticity that can be used for effective intervention. These insights greatly contribute to the field of AI transparency, addressing a pressing lack of efficiency limiting our ability to effectively steer model behavior.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.06225"
    }
  },
  {
    "id": 159,
    "title": "Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering Vectors",
    "tags": [],
    "primaryTag": "Application/Activation Steering",
    "date": "2024-10",
    "authors": [
      "Weixuan Wang",
      "Jingyuan Yang",
      "Wei Peng"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable performance across many tasks, yet aligning them with desired behaviors remains challenging. Activation intervention has emerged as an effective and economical method to modify the behavior of LLMs. Despite considerable interest in this area, current intervention methods exclusively employ a fixed steering vector to modify model activations, lacking adaptability to diverse input semantics. To address this limitation, we propose Semantics-Adaptive Dynamic Intervention (SADI), a novel method that constructs a dynamic steering vector to intervene model activations at inference time. More specifically, SADI utilizes activation differences in contrastive pairs to precisely identify critical elements of an LLM (i.e., attention heads, hidden states, and neurons) for targeted intervention. During inference, SADI dynamically steers model behavior by scaling element-wise activations based on the directions of input semantics. Experimental results show that SADI outperforms established baselines by substantial margins, improving task performance without training. SADI's cost-effectiveness and generalizability across various LLM backbones and tasks highlight its potential as a versatile alignment technique.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.12299"
    }
  },
  {
    "id": 160,
    "title": "Locating and Editing Factual Associations in GPT",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "2022",
    "authors": [
      "Kevin Meng",
      "David Bau",
      "Alex Andonian",
      "Yonatan Belinkov"
    ],
    "abstract": "",
    "urls": {
      "NIPS 2022": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html",
      "github": "https://github.com/kmeng01/rome"
    }
  },
  {
    "id": 161,
    "title": "Memory-Based Model Editing at Scale",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "2022",
    "authors": [
      "Eric Mitchell",
      "Charles Lin",
      "Antoine Bosselut",
      "Christopher D Manning",
      "Chelsea Finn"
    ],
    "abstract": "Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit’s intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model’s predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.",
    "urls": {
      "ICML 2022": "https://proceedings.mlr.press/v162/mitchell22a.html"
    }
  },
  {
    "id": 162,
    "title": "Editing models with task arithmetic",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "ICLR 2023": "https://openreview.net/forum?id=6t0Kwf8-jrj"
    }
  },
  {
    "id": 163,
    "title": "Mass-Editing Memory in a Transformer",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "",
    "authors": [],
    "abstract": "Anonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
    "urls": {
      "ICLR 2023": "https://openreview.net/forum?id=MkbcAHIYgyS"
    }
  },
  {
    "id": 164,
    "title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "ACL 2023 Findings": "https://aclanthology.org/2023.findings-acl.733"
    }
  },
  {
    "id": 165,
    "title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "",
    "authors": [],
    "abstract": "",
    "urls": {
      "ACL 2023": "https://aclanthology.org/2023.acl-long.300"
    }
  },
  {
    "id": 166,
    "title": "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "2023",
    "authors": [
      "Peter Hase",
      "Mohit Bansal",
      "Been Kim",
      "Asma Ghandeharioun"
    ],
    "abstract": "",
    "urls": {
      "NIPS 2023": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/3927bbdcf0e8d1fa8aa23c26f358a281-Abstract-Conference.html"
    }
  },
  {
    "id": 167,
    "title": "Editing Conceptual Knowledge for Large Language Models",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "2024-03",
    "authors": [
      "Xiaohan Wang",
      "Shengyu Mao",
      "Ningyu Zhang",
      "Shumin Deng",
      "Yunzhi Yao",
      "Yue Shen",
      "Lei Liang",
      "Jinjie Gu",
      "Huajun Chen"
    ],
    "abstract": "Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at this https URL.",
    "urls": {
      "arxiv 2403": "https://arxiv.org/abs/2403.06259"
    }
  },
  {
    "id": 168,
    "title": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "2024-06",
    "authors": [
      "Cheng-Hsun Hsueh",
      "Paul Kuo-Ming Huang",
      "Tzu-Han Lin",
      "Che-Wei Liao",
      "Hung-Chieh Fang",
      "Chao-Wei Huang",
      "Yun-Nung Chen"
    ],
    "abstract": "Knowledge editing is a rising technique for efficiently updating factual knowledge in large language models (LLMs) with minimal alteration of parameters. However, recent studies have identified side effects, such as knowledge distortion and the deterioration of general abilities, that have emerged after editing. Despite these findings, evaluating the pitfalls of knowledge editing often relies on inconsistent metrics and benchmarks, lacking a uniform standard. In response, this survey presents a comprehensive study of these side effects, providing a unified perspective on the challenges of knowledge editing in LLMs by conducting experiments with consistent metrics and benchmarks. Additionally, we review related works and outline potential research directions to address these limitations. Our survey highlights the limitations of current knowledge editing methods, emphasizing the need for a deeper understanding of the inner knowledge structures of LLMs and improved knowledge editing methods. To foster future research, we have released the complementary materials publicly in this https URL.",
    "urls": {
      "arxiv 2406": "https://arxiv.org/abs/2406.01436"
    }
  },
  {
    "id": 169,
    "title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "2024-10",
    "authors": [
      "Zhuoran Zhang",
      "Yongxiang Li",
      "Zijian Kan",
      "Keyuan Cheng",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "The locate-then-edit paradigm has shown significant promise for knowledge editing (KE) in Large Language Models (LLMs). While previous methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop factual recall tasks involving newly edited knowledge. In this paper, leveraging tools in mechanistic interpretability, we first identify that in multi-hop tasks, LLMs tend to retrieve knowledge with implicit subject information from deeper MLP layers, unlike single-hop tasks, which rely on shallow layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shallow layers with single-hop edit prompts, leaving deeper layers unchanged. To address this, we propose IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. Beyond single-hop editing prompts, IFMET further incorporates multi-hop editing prompts to locate and modify knowledge across different stages of reasoning. Experimental results demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks, overcoming the limitations of previous locate-then-edit methods",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.06331"
    }
  },
  {
    "id": 170,
    "title": "Keys to Robust Edits: from Theoretical Insights to Practical Advances",
    "tags": [],
    "primaryTag": "Application/Knowledge Editing",
    "date": "2024-10",
    "authors": [
      "Jianhao Yan",
      "Futing Wang",
      "Yun Luo",
      "Yafu Li",
      "Yue Zhang"
    ],
    "abstract": "Large language models (LLMs) have revolutionized knowledge storage and retrieval, but face challenges with conflicting and outdated information. Knowledge editing techniques have been proposed to address these issues, yet they struggle with robustness tests involving long contexts, paraphrased subjects, and continuous edits. This work investigates the cause of these failures in locate-and-edit methods, offering theoretical insights into their key-value modeling and deriving mathematical bounds for robust and specific edits, leading to a novel 'group discussion' conceptual model for locate-and-edit methods. Empirical analysis reveals that keys used by current methods fail to meet robustness and specificity requirements. To address this, we propose a Robust Edit Pathway (REP) that disentangles editing keys from LLMs' inner representations. Evaluations on LLaMA2-7B and Mistral-7B using the CounterFact dataset show that REP significantly improves robustness across various metrics, both in-domain and out-of-domain, with minimal trade-offs in success rate and locality. Our findings advance the development of reliable and flexible knowledge updating in LLMs.",
    "urls": {
      "arxiv 2410": "https://arxiv.org/abs/2410.09338"
    }
  },
  {
    "id": 171,
    "title": "The Internal State of an LLM Knows When It's Lying",
    "tags": [],
    "primaryTag": "Application/Hallucination",
    "date": "2023-04",
    "authors": [
      "Amos Azaria",
      "Tom Mitchell"
    ],
    "abstract": "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\% to 83\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",
    "urls": {
      "EMNLP 2023 Findings": "https://arxiv.org/abs/2304.13734"
    }
  },
  {
    "id": 172,
    "title": "Do Androids Know They're Only Dreaming of Electric Sheep?",
    "tags": [],
    "primaryTag": "Application/Hallucination",
    "date": "2023-12",
    "authors": [
      "Sky CH-Wang",
      "Benjamin Van Durme",
      "Jason Eisner",
      "Chris Kedzie"
    ],
    "abstract": "We design probes trained on the internal representations of a transformer language model to predict its hallucinatory behavior on three grounded generation tasks. To train the probes, we annotate for span-level hallucination on both sampled (organic) and manually edited (synthetic) reference outputs. Our probes are narrowly trained and we find that they are sensitive to their training domain: they generalize poorly from one task to another or from synthetic to organic hallucinations. However, on in-domain data, they can reliably detect hallucinations at many transformer layers, achieving 95% of their peak performance as early as layer 4. Here, probing proves accurate for evaluating hallucination, outperforming several contemporary baselines and even surpassing an expert human annotator in response-level detection F1. Similarly, on span-level labeling, probes are on par or better than the expert annotator on two out of three generation tasks. Overall, we find that probing is a feasible and efficient alternative to language model hallucination evaluation when model states are available.",
    "urls": {
      "arxiv 2312": "https://arxiv.org/abs/2312.17249"
    }
  },
  {
    "id": 173,
    "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection",
    "tags": [],
    "primaryTag": "Application/Hallucination",
    "date": "2024",
    "authors": [
      "Chao Chen",
      "Kai Liu",
      "Ze Chen",
      "Yi Gu",
      "Yue Wu",
      "Mingyuan Tao",
      "Zhihang Fu",
      "Jieping Ye"
    ],
    "abstract": "Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates for halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular, a simple yet effective \\textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.",
    "urls": {
      "ICLR 2024": "https://openreview.net/forum?id=Zj12nzlQbz"
    }
  },
  {
    "id": 174,
    "title": "TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space",
    "tags": [],
    "primaryTag": "Application/Hallucination",
    "date": "2024-02",
    "authors": [
      "Shaolei Zhang",
      "Tian Yu",
      "Yang Feng"
    ],
    "abstract": "Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM's knowledge potential. In this paper, we propose TruthX, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM's internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM's internal representations.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.17811"
    }
  },
  {
    "id": 175,
    "title": "Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension",
    "tags": [],
    "primaryTag": "Application/Hallucination",
    "date": "2024-02",
    "authors": [
      "Fan Yin",
      "Jayanth Srinivasa",
      "Kai-Wei Chang"
    ],
    "abstract": "We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.18048"
    }
  },
  {
    "id": 176,
    "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models",
    "tags": [],
    "primaryTag": "Application/Hallucination",
    "date": "2024-02",
    "authors": [
      "Hongbang Yuan",
      "Pengfei Cao",
      "Zhuoran Jin",
      "Yubo Chen",
      "Daojian Zeng",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse premise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating \\textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1%1\\% of the attention heads in the model yields a notable increase of nearly 20%20\\% of model performance.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.19103"
    }
  },
  {
    "id": 177,
    "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation",
    "tags": [],
    "primaryTag": "Application/Hallucination",
    "date": "2024-03",
    "authors": [
      "Shiqi Chen",
      "Miao Xiong",
      "Junteng Liu",
      "Zhengxuan Wu",
      "Teng Xiao",
      "Siyang Gao",
      "Junxian He"
    ],
    "abstract": "Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinations and serve as a practical solution for hallucination mitigation.",
    "urls": {
      "arxiv 2403": "http://arxiv.org/abs/2403.01548"
    }
  },
  {
    "id": 178,
    "title": "Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models",
    "tags": [],
    "primaryTag": "Application/Hallucination",
    "date": "2024-03",
    "authors": [
      "Weihang Su",
      "Changyue Wang",
      "Qingyao Ai",
      "Yiran HU",
      "Zhijing Wu",
      "Yujia Zhou",
      "Yiqun Liu"
    ],
    "abstract": "Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.",
    "urls": {
      "arxiv 2403": "https://arxiv.org/abs/2403.06448"
    }
  },
  {
    "id": 179,
    "title": "Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories",
    "tags": [],
    "primaryTag": "Application/Hallucination",
    "date": "2024-05",
    "authors": [
      "Tianlong Wang",
      "Xianfeng Jiao",
      "Yinghao Zhu",
      "Zhongzhi Chen",
      "Yifan He",
      "Xu Chu",
      "Junyi Gao",
      "Yasha Wang",
      "Liantao Ma"
    ],
    "abstract": "Recent studies have indicated that Large Language Models (LLMs) harbor an inherent understanding of truthfulness, yet often fail to consistently express it and generate false statements. This gap between \"knowing\" and \"telling\" poses a challenge for ensuring the truthfulness of generated content. Inspired by recent work on the practice of encoding human-interpretable concepts linearly within large language models, we treat truthfulness as a specially linearly encoded concept within LLMs, and introduce Adaptive Activation Steering (ACT), a tuning-free method that adaptively shifts LLM's activations in the \"truthful\" direction during inference. ACT addresses diverse categories of hallucinations by utilizing diverse truthfulness-related steering vectors and adjusting the steering intensity adaptively. Applied as an add-on across various models, ACT significantly improves truthfulness in LLaMA (↑\\uparrow 142%), LLaMA2 (↑\\uparrow 24%), Alpaca (↑\\uparrow 36%), Vicuna (↑\\uparrow 28%), LLaMA2-Chat (↑\\uparrow 19%), and LLaMA3(↑\\uparrow 34%). Furthermore, we verify ACT's scalability across larger models (13B, 33B, 65B), underscoring the adaptability of ACT to large-scale language models. Our code is available at this https URL.",
    "urls": {
      "arxiv 2406": "https://arxiv.org/abs/2406.00034"
    }
  },
  {
    "id": 180,
    "title": "Not all Layers of LLMs are Necessary during Inference",
    "tags": [],
    "primaryTag": "Application/Redundancy",
    "date": "2024-03",
    "authors": [
      "Siqi Fan",
      "Xin Jiang",
      "Xiang Li",
      "Xuying Meng",
      "Peng Han",
      "Shuo Shang",
      "Aixin Sun",
      "Yequan Wang",
      "Zhongyuan Wang"
    ],
    "abstract": "Due to the large number of parameters, the inference phase of Large Language Models (LLMs) is resource-intensive. However, not all requests posed to LLMs are equally difficult to handle. Through analysis, we show that for some tasks, LLMs can achieve results comparable to the final output at some intermediate layers. That is, not all layers of LLMs are necessary during inference. If we can predict at which layer the inferred results match the final results (produced by evaluating all layers), we could significantly reduce the inference cost. To this end, we propose a simple yet effective algorithm named AdaInfer to adaptively terminate the inference process for an input instance. AdaInfer relies on easily obtainable statistical features and classic classifiers like SVM. Experiments on well-known LLMs like the Llama2 series and OPT, show that AdaInfer can achieve an average of 17.8% pruning ratio, and up to 43% on sentiment tasks, with nearly no performance drop (<1%). Because AdaInfer does not alter LLM parameters, the LLMs incorporated with AdaInfer maintain generalizability across tasks.",
    "urls": {
      "arxiv 2403": "http://arxiv.org/abs/2403.02181"
    }
  },
  {
    "id": 181,
    "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
    "tags": [],
    "primaryTag": "Application/Redundancy",
    "date": "2024-03",
    "authors": [
      "Xin Men",
      "Mingyu Xu",
      "Qingyu Zhang",
      "Bingning Wang",
      "Hongyu Lin",
      "Yaojie Lu",
      "Xianpei Han",
      "Weipeng Chen"
    ],
    "abstract": "As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture.",
    "urls": {
      "arxiv 2403": "http://arxiv.org/abs/2403.03853"
    }
  },
  {
    "id": 182,
    "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
    "tags": [],
    "primaryTag": "Application/Redundancy",
    "date": "2024-03",
    "authors": [
      "Andrey Gromov",
      "Kushal Tirumala",
      "Hassan Shapourian",
      "Paolo Glorioso",
      "Daniel A. Roberts"
    ],
    "abstract": "How is knowledge stored in an LLM's weights? We study this via layer pruning: if removing a certain layer does not affect model performance in common question-answering benchmarks, then the weights in that layer are not necessary for storing the knowledge needed to answer those questions. To find these unnecessary parameters, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning. Surprisingly, with this method we find minimal degradation of performance until after a large fraction (up to half) of the layers are removed for some common open-weight models. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge. For our study, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single 40GB A100 GPU.",
    "urls": {
      "arxiv 2403": "http://arxiv.org/abs/2403.17887"
    }
  },
  {
    "id": 183,
    "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
    "tags": [],
    "primaryTag": "Application/Redundancy",
    "date": "2024",
    "authors": [
      "Vedang Lad",
      "Wes Gurnee",
      "Max Tegmark"
    ],
    "abstract": "We demonstrate and investigate the remarkable robustness of Large Language Models by deleting and swapping adjacent layers. We find that deleting and swapping interventions retain 72-95% of the original model's prediction accuracy without fine-tuning, whereas models with more layers exhibit more robustness. Based on the results of the layer-wise intervention and further experiments, we hypothesize the existence of four universal stages of inference across eight different models: detokenization, feature engineering, prediction ensembling, and residual sharpening. The first stage integrates local information, lifting raw token representations into higher-level contextual representations. Next is the iterative refinement of task and entity-specific features. Then, the second half of the model begins with a phase transition, where hidden representations align more with the vocabulary space due to specialized model components. Finally, the last layer sharpens the following token distribution by eliminating obsolete features that add noise to the prediction.",
    "urls": {
      "ICML 2024 MI Workshop": "https://openreview.net/forum?id=R5unwb9KPc"
    }
  },
  {
    "id": 184,
    "title": "The Hydra Effect: Emergent Self-repair in Language Model Computations",
    "tags": [],
    "primaryTag": "Component/General",
    "date": "2023-07",
    "authors": [
      "Thomas McGrath",
      "Matthew Rahtz",
      "Janos Kramar",
      "Vladimir Mikulik",
      "Shane Legg"
    ],
    "abstract": "We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.",
    "urls": {
      "arxiv 2307": "https://arxiv.org/abs/2307.15771"
    }
  },
  {
    "id": 185,
    "title": "Unveiling A Core Linguistic Region in Large Language Models",
    "tags": [],
    "primaryTag": "Component/General",
    "date": "2023-10",
    "authors": [
      "Jun Zhao",
      "Zhihao Zhang",
      "Yide Ma",
      "Qi Zhang",
      "Tao Gui",
      "Luhui Gao",
      "Xuanjing Huang"
    ],
    "abstract": "Brain localization, which describes the association between specific regions of the brain and their corresponding functions, is widely accepted in the field of cognitive science as an objective fact. Today's large language models (LLMs) possess human-level linguistic competence and can execute complex tasks requiring abstract knowledge and reasoning. To deeply understand the inherent mechanisms of intelligence emergence in LLMs, this paper conducts an analogical research using brain localization as a prototype. We have discovered a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. This core region exhibits significant dimension dependency, and perturbations to even a single parameter on specific dimensions can lead to a loss of linguistic competence. Furthermore, we observe that an improvement in linguistic competence does not necessarily accompany an elevation in the model's knowledge level, which might imply the existence of regions of domain knowledge that are dissociated from the linguistic region. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence. In the future, we will continue to investigate knowledge regions within LLMs and the interactions between them.",
    "urls": {
      "arxiv 2310": "http://arxiv.org/abs/2310.14928"
    }
  },
  {
    "id": 186,
    "title": "Exploring the Residual Stream of Transformers",
    "tags": [],
    "primaryTag": "Component/General",
    "date": "2023-12",
    "authors": [
      "Zeping Yu",
      "Sophia Ananiadou"
    ],
    "abstract": "Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify \"value neurons\" directly contributing to the final prediction, we propose a method for identifying \"query neurons\" which activate these \"value neurons\". Finally, we apply our methods to analyze six types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. The code is available on this https URL.",
    "urls": {
      "arxiv 2312": "http://arxiv.org/abs/2312.12141"
    }
  },
  {
    "id": 187,
    "title": "Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation",
    "tags": [],
    "primaryTag": "Component/General",
    "date": "2023-12",
    "authors": [
      "Randall Balestriero",
      "Romain Cosentino",
      "Sarath Shekkizhar"
    ],
    "abstract": "Large Language Models (LLMs) drive current AI breakthroughs despite very little being known about their internal representations. In this work, we propose to shed the light on LLMs inner mechanisms through the lens of geometry. In particular, we develop in closed form (i)(i) the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and (ii)(ii) the partition and per-region affine mappings of the feedforward (MLP) network of LLMs' layers. Our theoretical findings further enable the design of novel principled solutions applicable to state-of-the-art LLMs. First, we show that, through our geometric understanding, we can bypass LLMs' RLHF protection by controlling the embedding's intrinsic dimension through informed prompt manipulation. Second, we derive interpretable geometrical features that can be extracted from any (pre-trained) LLM, providing a rich abstract representation of their inputs. We observe that these features are sufficient to help solve toxicity detection, and even allow the identification of various types of toxicity. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in LLMs. Code: this https URL",
    "urls": {
      "arxiv 2312": "https://arxiv.org/abs/2312.01648"
    }
  },
  {
    "id": 188,
    "title": "Explorations of Self-Repair in Language Models",
    "tags": [],
    "primaryTag": "Component/General",
    "date": "2024-02",
    "authors": [
      "Cody Rushing",
      "Neel Nanda"
    ],
    "abstract": "Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.",
    "urls": {
      "arxiv 2402": "http://arxiv.org/abs/2402.15390"
    }
  },
  {
    "id": 189,
    "title": "Massive Activations in Large Language Models",
    "tags": [],
    "primaryTag": "Component/General",
    "date": "2024-02",
    "authors": [
      "Mingjie Sun",
      "Xinlei Chen",
      "J. Zico Kolter",
      "Zhuang Liu"
    ],
    "abstract": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers. Code is available at this https URL.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.17762"
    }
  },
  {
    "id": 190,
    "title": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions",
    "tags": [],
    "primaryTag": "Component/General",
    "date": "2024-02",
    "authors": [
      "Clement Neo",
      "Shay B. Cohen",
      "Fazl Barez"
    ],
    "abstract": "Understanding the inner workings of large language models (LLMs) is crucial for advancing their theoretical foundations and real-world applications. While the attention mechanism and multi-layer perceptrons (MLPs) have been studied independently, their interactions remain largely unexplored. This study investigates how attention heads and next-token neurons interact in LLMs to predict new words. We propose a methodology to identify next-token neurons, find prompts that highly activate them, and determine the upstream attention heads responsible. We then generate and evaluate explanations for the activity of these attention heads in an automated manner. Our findings reveal that some attention heads recognize specific contexts relevant to predicting a token and activate a downstream token-predicting neuron accordingly. This mechanism provides a deeper understanding of how attention heads work with MLP neurons to perform next-token prediction. Our approach offers a foundation for further research into the intricate workings of LLMs and their impact on text generation and understanding.",
    "urls": {
      "arxiv 2402": "https://arxiv.org/abs/2402.15055"
    }
  },
  {
    "id": 191,
    "title": "Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics",
    "tags": [],
    "primaryTag": "Component/General",
    "date": "2024-03",
    "authors": [
      "Zhu Liu",
      "Cunliang Kong",
      "Ying Liu",
      "Maosong Sun"
    ],
    "abstract": "Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance via the hidden states for the last meaningless symbols, such as punctuation, in the prompting strategy. Our codes are available at this https URL.",
    "urls": {
      "arxiv 2403": "https://arxiv.org/abs/2403.01509"
    }
  },
  {
    "id": 192,
    "title": "The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models",
    "tags": [],
    "primaryTag": "Component/General",
    "date": "2024-03",
    "authors": [
      "Zhu Liu",
      "Cunliang Kong",
      "Ying Liu",
      "Maosong Sun"
    ],
    "abstract": "Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance via the hidden states for the last meaningless symbols, such as punctuation, in the prompting strategy. Our codes are available at this https URL.",
    "urls": {
      "arxiv 2403": "https://arxiv.org/abs/2403.01509"
    }
  }
]